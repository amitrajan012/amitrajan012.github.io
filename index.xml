<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Amit Rajan</title>
    <link>https://amitrajan012.github.io/</link>
    <description>Recent content on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Jun 2022 23:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Linear Models for Regression - Evidence Approximation &amp; Limitations of Fixed Basis Function</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_6/</link>
      <pubDate>Mon, 27 Jun 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_6/</guid>
      <description>3.5 The Evidence Approximation In a fully Bayesian treatment of the linear basis function model, we would introduce prior distributions over the hyperparameters $\alpha$ and $\beta$ and make predictions by marginalizing with respect to these hyperparameters as well as with respect to the parameters $W$. However, although we can integrate analytically over either $W$ or over the hyperparameters, the complete marginalization over all of these variables is analytically intractable. Our goal is to find the predictive distribution for each of the models $p(t|X,M_i)$.</description>
    </item>
    
    <item>
      <title>Linear Models for Regression - Bayesian Model Comparison</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_5/</link>
      <pubDate>Sun, 26 Jun 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_5/</guid>
      <description>3.4 Bayesian Model Comparison Here we consider the problem of model selection from a Bayesian perspective. The Bayesian view of model comparison simply involves the use of probabilities to represent uncertainty in the choice of model. Suppose we wish to compare a set of $L$ models ${M_i}$ where $i = 1,&amp;hellip;,L$. Here a model refers to a probability distribution over the observed data $D$. We shall suppose that the data is generated from one of these models but we are uncertain which one.</description>
    </item>
    
    <item>
      <title>Linear Models for Regression - Bayesian Linear Regression</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_4/</link>
      <pubDate>Sat, 25 Jun 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_4/</guid>
      <description>3.3 Bayesian Linear Regression One of the problems with frequentist approach and using maximum likelihood estimator is the issue of deciding the appropriate model complexity for the particular problem, which cannot be decided simply by maximizing the likelihood function, because this always leads to excessively complex models and over-fitting. We therefore turn to a Bayesian treatment of linear regression, which will avoid the over-fitting problem of maximum likelihood, and which will also lead to automatic methods of determining model complexity using the training data alone.</description>
    </item>
    
    <item>
      <title>Linear Models for Regression - Bias-Variance Decomposition</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_3/</link>
      <pubDate>Thu, 23 Jun 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_3/</guid>
      <description>3.2 Bias-Variance Decomposition We have seen that the use of maximum likelihood, or equivalently least squares, can lead to severe over-fitting if complex models are trained using data sets of limited size. However, limiting the number of basis functions in order to avoid over-fitting has the side effect of limiting the flexibility of the model to capture interesting and important trends in the data. The phenomenon of over-fitting is really an unfortunate property of maximum likelihood and does not arise when we marginalize over parameters in a Bayesian setting.</description>
    </item>
    
    <item>
      <title>Linear Models for Regression - Linear Basis Function Models : Part 2</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_2/</link>
      <pubDate>Tue, 21 Jun 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_2/</guid>
      <description>3.1.2 Geometry of Least Squares The least square problem can be geometrically interpreted as follows. Let the target veactor $t$ (having output for $N$ data points) spans a $N$-dimensional space. Each basis vector $\phi_j(X_n)$ will also be present in the same space. If the number $M$ of basis functions is smaller than the number of data points $N$, then the $M$ basis vectors will span a subspace $S$ of dimensionality $M$.</description>
    </item>
    
    <item>
      <title>Linear Models for Regression - Linear Basis Function Models : Part 1</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_1/</link>
      <pubDate>Mon, 20 Jun 2022 22:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-3-linear-models-for-regression_1/</guid>
      <description>Linear regression model has the property of being linear functions of adjustable parameters. We can add more complexity in the linear regression models by taking linear combinations of a fixed set of nonlinear functions of the input variables, known as basis functions. In the modeling process, giveb $x$, we have to predict $t$ which can be predicted as $y(x)$. Form a probabilistic prespective, we aim to model the predictive distribution $p(t|x)$ as this expresses the uncertainty about the value of $t$ for each value of $x$.</description>
    </item>
    
    <item>
      <title>Probability Distributions - Nonparametric Methods</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_9/</link>
      <pubDate>Sun, 19 Jun 2022 20:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_9/</guid>
      <description>2.5 Nonparametric Methods Till now to model the data, we have choosen distributions which are governed by some fixed small number of parameters. This is called as parametric approach to density modeling. The chosen density might be a poor model of the distribution that generates the data, which can result in poor predictive performance. In nonparametric approach, we make fewer assumptions about the form of distribution.
One common nonparametric approach is histogram density models.</description>
    </item>
    
    <item>
      <title>Probability Distributions - The Exponential Family</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_8/</link>
      <pubDate>Sat, 18 Jun 2022 19:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_8/</guid>
      <description>2.4 The Exponential Family Exponential family of distributions over $X$ with parameters $\eta$ is defined as
$$\begin{align} p(X|\eta) = h(X)g(\eta)exp[\eta^Tu(X)] \end{align}$$
Here $\eta$ is the natural parameter of the distribution and $u(X)$ is some function of $X$. $g(\eta)$ ensures that the distribution is normalized and satisfies
$$\begin{align} g(\eta) \int h(X)exp[\eta^Tu(X)] dX = 1 \end{align}$$
Bernoulli distribution is a common exponential distribution. It is given as
$$\begin{align} p(x|\mu) = Bern(x|\mu) = \mu^x(1-\mu)^{1-x} = exp[x\ln \mu + (1-x)\ln(1-\mu)] \end{align}$$</description>
    </item>
    
    <item>
      <title>Probability Distributions - The Gaussian Distribution: Part 5</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_7/</link>
      <pubDate>Fri, 17 Jun 2022 19:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_7/</guid>
      <description>2.3.8 Periodic Variables Periodic variables usually repeats its behaviour after a set amount of time. Let us assume a periodic variable represented as an angle from the x-axis by $D={\theta_1, \theta_2, &amp;hellip;, \theta_N}$, where $\theta$ is measured in radians. The mean and variance of these data points will depend on the choice of origin and the axis. To find the invariant measure of the mean, we denote these observations as the points on the unit circle and can be described as a two-dimensional unit vectors $X_1,X_2,&amp;hellip;,X_N$ where $||X_n|| = 1$ for $n=1,2,&amp;hellip;,N$ as shown in the following figure.</description>
    </item>
    
    <item>
      <title>Probability Distributions - The Gaussian Distribution: Part 4</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_6/</link>
      <pubDate>Thu, 16 Jun 2022 19:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_6/</guid>
      <description>2.3.6 Bayesian Inference for the Gaussian We can use Bayesian treatment to derive the point estimates for the mean and variance of the Gaussian by introducing prior distributions over these parameters. For a single ranodm variable, let us suppose that the variance $\sigma^2$ is known and we have to determine the mean $\mu$ given $N$ data points $X={x_1,x_2,&amp;hellip;,x_N}$. Under the assumption of independence, the likelihood function is give as
$$\begin{align} p(X|\mu) = \prod_{n=1}^{N} p(x_n|\mu) = \frac{1}{(2\pi\sigma^2)^{N/2}} exp \bigg(-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_n - \mu)^2\bigg) \end{align}$$</description>
    </item>
    
    <item>
      <title>Probability Distributions - The Gaussian Distribution: Part 3</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_5/</link>
      <pubDate>Wed, 15 Jun 2022 19:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_5/</guid>
      <description>2.3.3 Bayes&amp;rsquo; Theorem for Gaussian Variables The applicatio of Bayes&amp;rsquo; theorem in the Gaussian setting is when we have Gaussian marginal distribution $p(X)$ and a Gaussian conditional distribution $p(Y|X)$ and we wish to find the marginal and consitional distribution $p(Y)$ and $p(X|Y)$. It should be noted that the mean and covariace of the marginal distribution $p(X)$ are constant with respect to $X$. The covarince of the conditional distribution $p(Y|X)$ is constant with respect to $X$ but its mean is a linear function of $X$.</description>
    </item>
    
    <item>
      <title>Probability Distributions - The Gaussian Distribution: Part 2</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_4/</link>
      <pubDate>Tue, 14 Jun 2022 19:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_4/</guid>
      <description>2.3.1 Conditional Gaussian Distribution If two sets of variables are jointly Gaussian, then the conditional distribution of one set conditioned on the othre is Gaussian. The marginal distribution of either set is also Gaussian. Let $X$ is a $D$-dimensional vector with Gaussian distribution $N(X|\mu,\Sigma)$. $X$ is partitioned into two disjoint subsets $X_a,X_b$. Without loss of generality we can assume thet $X_a$ forms the first $M$ components of $X$ and $X_b$ the remaining $D-M$, such that</description>
    </item>
    
    <item>
      <title>Probability Distributions - The Gaussian Distribution: Part 1</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_3/</link>
      <pubDate>Mon, 13 Jun 2022 19:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_3/</guid>
      <description>2.3 The Gaussian Distribution Gaussian Distribution is a widely used model for the distribution of continous variables. For a single variable $x$, the gaussian distribution is given as
$$\begin{align} N(x|\mu,\sigma^2) = \frac{1}{(2\pi\sigma^2)^{1/2}} exp \bigg[\frac{-1}{2\sigma^2} (x-\mu)^2\bigg] \end{align}$$
where $\mu$ and $\sigma^2$ are mean and variance respectively. For a $D$ dimensional vector $X$, the multivariate gaussian distribution takes the form
$$\begin{align} N(X|\mu,\Sigma) = \frac{1}{(2\pi)^{D/2}}\frac{1}{|\Sigma|^{1/2}} exp \bigg[\frac{-1}{2} (X-\mu)^T\Sigma^{-1}(X-\mu)\bigg] \end{align}$$
where $\mu$ is a $D$ dimensional mean vector and $\Sigma$ is a $D\times D$ dimensional covariance matrix with $|\Sigma|$ being its determinant.</description>
    </item>
    
    <item>
      <title>Probability Distributions - Multinomial Variables</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_2/</link>
      <pubDate>Sun, 12 Jun 2022 17:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_2/</guid>
      <description>2.2 Multinomial Variables A multinomila variable can be in any of $K$ states instead of just $2$ (in case of a binary variable). For example, a multinomial variable having $K=5$ states can be represented as $x=(0,0,1,0,0)^T$ where it is in a state where $x_3=1$. This vector will satisfy $\sum_{k=1}^K = 1$. If we denote the probability of $x_k=1$ by $\mu_k$, the distribution of $x$ is given as:
$$\begin{align} p(x|\mu) = \prod_{k=1}^{K} \mu_k^{x_k} \end{align}$$</description>
    </item>
    
    <item>
      <title>Probability Distributions - Binary Variables</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_1/</link>
      <pubDate>Sat, 11 Jun 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-2-probability-distributions_1/</guid>
      <description>One goal of pattern recognition is to model the probability distribution $p(x)$ of a random variable $x$ given the finite set of points $x_1, x_2, &amp;hellip;, x_N$. This problem is known as density estimation. For simplicity, we can assume that the points are independent and identically distributed. There can be infinitely many distributions that can give rise to the given data points with any distribution that is non-zero at the points $x_1, x_2, &amp;hellip;, x_N$ as a potential candidate.</description>
    </item>
    
    <item>
      <title>Introduction - Information Theory</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_5/</link>
      <pubDate>Fri, 10 Jun 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_5/</guid>
      <description>1.6 Information Theory Some amount of information is recieved when we observe the value of a discrete random variable $x$. If a highly improbable event has occured, the amount of information received is higher. For an event which was certain to happen, no amount of information is received. Hence, the measure of information $h(x)$ will therefore depend on the probability distribution $p(x)$ and will be a monotnic function of $p(x)$. For two unrelated events $x$ and $y$, the information gain from observing both of them $h(x,y) = h(x) + h(y)$ is sum of the information gained from each of them separately.</description>
    </item>
    
    <item>
      <title>Introduction - Decision Theory</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_4/</link>
      <pubDate>Sat, 04 Jun 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_4/</guid>
      <description>1.5 Decision Theory The idea behind decision theorey is to convert the model probabilities (mainly for classification problem) to a decision. Given an input vector $x$ with the corresponding output $t$, the joint probability distribution $p(x,t)$ will provide the complete summary of uncertainity associated with these variables. Determination of $p(x,t)$ from the training data is a difficult problem and hence in a practical setting, we are more intersted in taking decisions based on probable value of $t$ for a given $x$.</description>
    </item>
    
    <item>
      <title>Introduction - Model Selection &amp; Curse of Dimensionality</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_3/</link>
      <pubDate>Sun, 29 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_3/</guid>
      <description>1.3 Model Selection One of the commonly used technique for model selection is cross-validation. In a $S-fold$ cross-validation, training set is divided into $S$ equal subests where one of the subset is used for model vaidation and the remaining $S-1$ sets will be used for model training. This means that a total fraction of $\frac{S-1}{S}$ of dataset is used for model training. For a scarce dataset, it is common practice to set $S=N$, which is called as leave-one-out cross-validation technique.</description>
    </item>
    
    <item>
      <title>Introduction - Probability Theory</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_2/</link>
      <pubDate>Mon, 23 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_2/</guid>
      <description>1.2 Probability Theory Consider two random variables $X,Y$ where $X$ can take any of the values $x_i$ where $i=1,2,&amp;hellip;,M$ and $Y$ can take the values $y_j$ where $j=1,2,&amp;hellip;,L$. In a total of $N$ trials, both $X,Y$ are sampled and the number of trials for which $X=x_i,Y=y_j$ is $n_{ij}$. The number of trials in which $X=x_i$ is $c_i$ and $Y=y_j$ is $r_j$ respectively. Then, the joint probability of $X$ taking the value $x_i$ and $Y$ taking the value $y_j$ is given as:</description>
    </item>
    
    <item>
      <title>Introduction - Polynomial Curve Fitting</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_1/</link>
      <pubDate>Fri, 20 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-1-introduction_1/</guid>
      <description>Any pattren recognition or machine learning task can be primarily divided into two categories: Supervised and Unsupervised Learning. In a supervised machine learning problem, we have the input and corresponding desired output. For any supervised learning problem, the aim of the pattern recognition algorithm is to come up with an algorithm or model which can predict the output given the input. Based on the output, the supervised learning problem can be divided into two categories: Classification (when we have a finite number of discrete output) and Regression (if the desired output consists of one or more continuous variables).</description>
    </item>
    
    <item>
      <title>Left, Right  and Pseudo Inverses</title>
      <link>https://amitrajan012.github.io/post/chapter-28-left-and-right-inverses-and-pseudoinverse/</link>
      <pubDate>Sun, 15 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-28-left-and-right-inverses-and-pseudoinverse/</guid>
      <description>28.1 Left and Right Inverses For any matrix $A$, there are four fundamental subspaces: Row Space, Null Space, Column Space, Null Space of $A^T$. Row Space and Null Space combined span entire $\mathbb{R}^n$ for a $m \times n$ matrix $A$. Column Space and Null Space of $A^T$ combined span entire $\mathbb{R}^m$ for a $m \times n$ matrix $A$. For the inverse of the matrix to exist, it&amp;rsquo;s rank should be $r=m=n$ whtere the dimension of the matrix is $m \times n$.</description>
    </item>
    
    <item>
      <title>Linear Transformations, Change of Basis and Image Compression</title>
      <link>https://amitrajan012.github.io/post/chapter-27-linear-transformations-and-their-matrices-change-of-basis-and-image-compression/</link>
      <pubDate>Thu, 12 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-27-linear-transformations-and-their-matrices-change-of-basis-and-image-compression/</guid>
      <description>27.1 Linear Transformations When we are dealing with coordinates, every linear transformation leads us to a matrix. For any vector $v$ and $w$, linear transformation $T$ follows following properties:
 $T(v+w) = T(v) + T(w)$ $T(cv) = cT(v)$ $T(0) = T(0)$, derived from tha above two properties  Below are some examples and non-examples of linear transformation:
Example 1: Projection In a two-dimensional space $\mathbb{R}^2$, a projection of a vector $v$ on a line is linear transformation and can be denoted as $T:\mathbb{R}^2 \to \mathbb{R}^2$.</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://amitrajan012.github.io/post/chapter-26-singular-value-decomposition/</link>
      <pubDate>Mon, 09 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-26-singular-value-decomposition/</guid>
      <description>26.1 Singular Value Decomposition If any matrix $A$ can be decomposed as $A = U\Sigma V^T$, where $\Sigma$ is a diagonal matrix and $U,V$ are orthogonal matrices, this is called as Singular Value Decomposition (SVD). One of the examples of SVD is for a symmetric positive definite matrix $A$, we know that $A = Q\Lambda Q^T$, where $\Lambda$ is diagonal and $Q$ is orthogonal.
For a matrix $m \times n$ matrix $A$, let the row-space be entire $\mathbb{R}^n$ and the column-space be entire $\mathbb{R}^m$.</description>
    </item>
    
    <item>
      <title>Similar Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter-25-similar-matrices-and-jordan-form/</link>
      <pubDate>Fri, 06 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-25-similar-matrices-and-jordan-form/</guid>
      <description>25.1 Similar Matrices One of the major generator of a positive definite matrix is when we square one. In a nutsheel, the matrix $A^TA$ is always positive definite. To prove this, let $A$ be a $m \times n$ rectangular matrix. Then $A^TA$ is a square symmetric matrix. If we evaluate the expression $x^T(A^TA)x$, we get $x^T(A^TA)x = (x^TA^T)(Ax) = (Ax)^T(Ax) = |Ax|^2 &amp;gt; 0$ if $Ax$ is non-zero. Another important thing to note is if matrix $A, B$ are positive definite, $A+B$ is positive definite.</description>
    </item>
    
    <item>
      <title>Positive Definite Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter-24-positive-definite-matrices-and-minima/</link>
      <pubDate>Mon, 02 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-24-positive-definite-matrices-and-minima/</guid>
      <description>24.1 Positive Definite Matrices These are the complete tests for a $2 \times 2$ matrix $A = \begin{bmatrix} a &amp;amp; b \\ b &amp;amp; c \end{bmatrix}$ for being Positive Definite:
 Both the eigenvalues should be positive: $\lambda_1 &amp;gt; 0;\lambda_2 &amp;gt; 0$ All the sub-determinants should be positive: $a &amp;gt; 0; ac - b^2 &amp;gt; 0$ Pivots should be positive: $a&amp;gt;0;\frac{ac-b^2}{a} &amp;gt; 0$ $x^TAx &amp;gt; 0;\forall x$  The matrix for which any of these conditions holds with equality instead are called as positive semi-definite matrices.</description>
    </item>
    
    <item>
      <title>Complex Matrices and Fourier Transform</title>
      <link>https://amitrajan012.github.io/post/chapter-23-complex-matrices-and-fast-fourier-transform/</link>
      <pubDate>Fri, 29 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-23-complex-matrices-and-fast-fourier-transform/</guid>
      <description>23.1 Complex Vectors/Matrices Let $z = \begin{bmatrix} z_1 \\ z_2 \\ &amp;hellip; \\ z_n \end{bmatrix}$ be a complex vector in $C^n$. The expresson $z^Tz$ doesn&amp;rsquo;t represent the length of the vector $z$. Instead it&amp;rsquo;s length is represented by $\overline{z}^Tz$. $\overline{z}^T$ is also called as $z^H$ and is called as Hermitian of a matrix. Hence, the length of a complex vector is $z^Hz = |z_1|^2 + |z_2|^2 + &amp;hellip; + |z_n|^2$. Similarly, for a complex matrix $A$ to be symmetric, $A^H = A$ with diagonal elements being real.</description>
    </item>
    
    <item>
      <title>Symmetric Matrices and Positive Definiteness</title>
      <link>https://amitrajan012.github.io/post/chapter_22_symmetric_matrices_and_positive_definiteness/</link>
      <pubDate>Tue, 26 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_22_symmetric_matrices_and_positive_definiteness/</guid>
      <description>22.1 Symmetric Matrices For a Symmetric Matrix $A$, $A = A^T$. Eigenvalues of real Symmetric Matrices are real and eigenvectors are perpendicular. Any matrix $A$ can be written as $A = S\Lambda S^{-1}$. For a symmetric matrix $A$, this relationship reduces to $A = Q \Lambda Q^{-1}$ as $S$, which is the eigenvector matrix has orthonormal eigenvectors. For an orthonormal matrix, $Q^{-1} = Q^T$ and hence $A = Q\Lambda Q^T$. This is called as the Spectral Theorem in mathematics.</description>
    </item>
    
    <item>
      <title>Markov Matrices and Fourier Series</title>
      <link>https://amitrajan012.github.io/post/chapter_21_markov_matrices_and_fourier_series/</link>
      <pubDate>Fri, 22 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_21_markov_matrices_and_fourier_series/</guid>
      <description>21.1 Markov Matrices Markov Matrices have the following properties:
 All entries $\geq 0$ Sum of the entries in a column equal $1$  A markov matrix will always have an eigenvalue of $1$. Apart from this, all other eigenvalues will be $\leq 1$.
Let us consider the difference equation $u_k = A^ku_0$ where we represent $u_0$ as the combinations of eigenvectors, i.e. $u_0 = c_1x_1 + c_2x_2 + &amp;hellip; + c_nx_n = Sc$.</description>
    </item>
    
    <item>
      <title>Differential Equations and Matrix Exponentials</title>
      <link>https://amitrajan012.github.io/post/chapter_20_differential_equations_and_expat/</link>
      <pubDate>Wed, 20 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_20_differential_equations_and_expat/</guid>
      <description>20.1 Differential Equations $\frac{du}{dt} = Au$ Example: Let the system of differential equation to be solved is: $\frac{du_1}{dt} = -u_1 + 2u_2; \frac{du_2}{dt} = u_1 - 2u_2$ with initial condition of $u(0) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$. The matrix $A$ representing the coefficients of the equation is $A = \begin{bmatrix} -1 &amp;amp; 2 \\ 1 &amp;amp; -2 \end{bmatrix}$. The eigenvalues of the matrix $A$ satisfies the equation $\lambda_1 + \lambda_2 = -3; \lambda_1 \times \lambda_2 = 0$, i.</description>
    </item>
    
    <item>
      <title>Diagonalization and Powers of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_19_diagonalization_and_powers_of_a_matrix/</link>
      <pubDate>Sun, 17 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_19_diagonalization_and_powers_of_a_matrix/</guid>
      <description>19.1 Diagonalization of a Matrix Suppose for a given matrix $A$, we have $n$ linearlly independent eigenvectros and we put them in a matrix $S$. Then $AS = A \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix}$, where each $x_i$ is the eigenvector. But for each of the eigenvectors, $Ax_i = \lambda_ix_i$. Hence, $AS = A \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix} = \begin{bmatrix} \lambda_1x_1 &amp;amp; \lambda_2x_2 &amp;amp; &amp;hellip; &amp;amp; \lambda_nx_n \end{bmatrix} = \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix}diag(\lambda_i) = S\Lambda$.</description>
    </item>
    
    <item>
      <title>Eigenvalues and Eigenvectors</title>
      <link>https://amitrajan012.github.io/post/chapter_18_eigenvalues_and_eigenvectors/</link>
      <pubDate>Thu, 14 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_18_eigenvalues_and_eigenvectors/</guid>
      <description>18.1 Eigenvalues and Eigenvectors The usual function of a matrix is to act on a vector like a function, i.e. in goes a vector $x$, out comes a vector $Ax$. For a specific matrix $A$, if $Ax$ is parallel to $x$ i.e. $Ax = \lambda x$, the vectors $x$ are called as eigenvectors. The constant $\lambda$ is called as eigenvalue.
The vector $x$ and constant $\lambda$ satisfying the equation $Ax = \lambda x$ are eigenvectors and eigenvalues.</description>
    </item>
    
    <item>
      <title>Formula for $A^{-1}$ and Cramer&#39;s Rule</title>
      <link>https://amitrajan012.github.io/post/chapter_17_cramers_rule_inverse_matrix_and_volume/</link>
      <pubDate>Sun, 10 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_17_cramers_rule_inverse_matrix_and_volume/</guid>
      <description>17.1 Formula for $A^{-1}$ $A^{-1}$ can be given as $A^{-1} = \frac{1}{|A|}C^T$, where $C^T$ is the matrix of cofactors transposed. To verify this formula, we have to check that $AA^{-1}=I$, or $AC^T=|A|I$. If we expand the left hand side, we get
$$\begin{align} \begin{bmatrix} a_{11} &amp;amp; &amp;hellip; &amp;amp; a_{1n}\\ : &amp;amp; : &amp;amp; :\\ a_{n1} &amp;amp; &amp;hellip; &amp;amp; a_{nn} \end{bmatrix}\begin{bmatrix} C_{11} &amp;amp; &amp;hellip; &amp;amp; C_{n1}\\ : &amp;amp; : &amp;amp; :\\ C_{1n} &amp;amp; &amp;hellip; &amp;amp; C_{nn} \end{bmatrix}=\begin{bmatrix} |A| &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; |A| &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; |A| \end{bmatrix}=|A|I \end{align}$$</description>
    </item>
    
    <item>
      <title>Determinant and Cofactors</title>
      <link>https://amitrajan012.github.io/post/chapter_16_determinant_formulas_and_cofactors/</link>
      <pubDate>Thu, 07 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_16_determinant_formulas_and_cofactors/</guid>
      <description>16.1 Formula for $|A|$ For a $2 \times 2$ matrix $A$, the formula for $|A|$ can be derived as follows:
$$\begin{align} |A| = \begin{vmatrix} a &amp;amp; b \\ c &amp;amp; d \end{vmatrix}= \begin{vmatrix} a &amp;amp; 0 \\ c &amp;amp; d \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ c &amp;amp; d \end{vmatrix} \end{align}$$
$$\begin{align} =\begin{vmatrix} a &amp;amp; 0 \\ c &amp;amp; 0 \end{vmatrix}+\begin{vmatrix} a &amp;amp; 0 \\ 0 &amp;amp; d \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ c &amp;amp; 0 \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ 0 &amp;amp; d \end{vmatrix} \end{align}$$</description>
    </item>
    
    <item>
      <title>Determinant</title>
      <link>https://amitrajan012.github.io/post/chapter_15_properties_of_determinants/</link>
      <pubDate>Tue, 05 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_15_properties_of_determinants/</guid>
      <description>15.1 Determinant Every square matrix has a number assosiated with it can we call this number as determinant, often denoted as $det(A) = |A|$ for matrix $A$.
Prperties of determinant is as follows:
  $|I|=1$
  Row exchange reverses the sign of the determinant:
  Permutation Matrices are derived by row exchange of Identity Matrix. Hence, $|P|= \pm1$.
 (a) For any square matrix $A$, $\begin{vmatrix} ta &amp;amp; tb \\ c &amp;amp; d \end{vmatrix}=t\begin{vmatrix} a &amp;amp; b \\ c &amp;amp; d \end{vmatrix}$</description>
    </item>
    
    <item>
      <title>Orthonormal Vectors, Orthogonal Matrices and Gram-Schmidt Method</title>
      <link>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</link>
      <pubDate>Sat, 02 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</guid>
      <description>14.1 Orthonormal Vectors &amp;amp; Orthogonal Matrices A set of Orthonormal Vectors can be defined as:
$$\begin{align} q_i^Tq_j = \begin{cases} 0 ,&amp;amp; \text{if } i \neq j \\ 1 ,&amp;amp; \text{if } i=j \end{cases} \end{align}$$
When these set of $n$ orthonormal vectors are put into a matrix $Q$ such that $Q = \begin{bmatrix} q_1 &amp;amp; q_2 &amp;amp; &amp;hellip; &amp;amp; q_n \end{bmatrix}$, then we get $Q^TQ = I$. It should be noted that $Q$ doesn&amp;rsquo;t have to be a square matrix.</description>
    </item>
    
    <item>
      <title>Projection Matrices and Least Squares</title>
      <link>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</link>
      <pubDate>Wed, 30 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</guid>
      <description>13.1 Projection Matrices Let us look at the two extreme cases while taking the projection of vector $b$ onto the plane represented by matrix $A$. The projection matrix $P$ is given as: $P = A(A^TA)^{-1}A^T$.
  Case 1: When $b$ is $\perp$ to the column space of $A$, it&amp;rsquo;s projection $p=0$. This means that $b$ lies in the null space of $A^T$, i.e. $A^Tb=0$. Hence, $p = Pb = A(A^TA)^{-1}A^Tb = 0$.</description>
    </item>
    
    <item>
      <title>Projection of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</link>
      <pubDate>Sat, 26 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</guid>
      <description>12.1 Projections in one-dimensional Space Given two vectors $a,b$ in a plane, the projection of $b$ onto $a$ is shown below. The projection $p$ will be a multiple of $a$ and the error vector $e$ will be orthogonal to $a$. Error vector can be denoted as: $e = b - p$. As $e \perp a$, we can say that: $a^Te =0 \implies a^T(b - p) = 0 \implies a^T(b - xa) = 0 \implies xa^Ta = a^Tb \implies x = \frac{a^Tb}{a^Ta}$.</description>
    </item>
    
    <item>
      <title>Orthogonal Vectors and Orthogonal Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</link>
      <pubDate>Wed, 23 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</guid>
      <description>11.1 Orthogonal Vectors Orthogonal means perpendicular. For two vectors $x,y$, they are orthogonal if and only if $x^Ty=0$. As per Pythagoras Theorem, the test for orthogonality is: $\lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2$.
We can conclude the dot product condition from Pythoagoras Theorem as follows:
$$\begin{align} \lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2 \\ x^Tx + y^Ty = (x+y)^T(x+y) \\ x^Tx + y^Ty = x^Tx + y^Ty + x^Ty + y^Tx \\ x^Ty + y^Tx = 0 \\ 2x^Ty = 0 \\ x^Ty = 0 \end{align}$$</description>
    </item>
    
    <item>
      <title>Graphs, Networks and Incidence Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</link>
      <pubDate>Mon, 21 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</guid>
      <description>10.1 Graphs, Networks and Incidence Matrices: Graphs consist of nodes and edges. For example, in the attached figure, the graph has $n=4$ nodes and $m=5$ edges. The graph can be interpreted as a circuit where nodes represent the points from which current flows and edges with the arrow represent the direction of its flow.
The above graph can be represented using a matrix, called as Incidence Matrix. The edges of the graph are represented using rows where entry at each column (one columnn for each of the node) index represents the start and end of the edge.</description>
    </item>
    
    <item>
      <title>Matrix Spaces</title>
      <link>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</link>
      <pubDate>Thu, 17 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</guid>
      <description>9.1 Matrix Spaces The idea of vector spaces can be extended to matrices as well as far as they follow the following properties
 If $A \in S$ then $cA \in S$ If $A \in S; B \in S$ then $A+B \in S$  It should be noted that the matrix multiplication doesn&amp;rsquo;t need to belong to the same matrix space.
For a matrix $M$, some of the examples of matrix spaces are: Upper Triangular Mtrices, Symmetric Matrices, Diagonal Matrices etc.</description>
    </item>
    
    <item>
      <title>Four Fundamental Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_8-_the_four_fundamental_subspaces/</link>
      <pubDate>Tue, 15 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_8-_the_four_fundamental_subspaces/</guid>
      <description>8.1 Four Fundamental Subspaces The four fundamental subspaces for a $m \times n$ matrix $A$ are as follows:
 Column Space $C(A)$ in $\mathbb{R}^m$ Null Space $N(A)$ in $\mathbb{R}^n$: Solution to $Ax=0$ Row Space $C(A^T)$ in $\mathbb{R}^n$: All combinations of the rows of $A$ or we can say that all combinations of the columns of $A^T$ Left Null Space of $A^T$ $N(A^T)$ in $\mathbb{R}^m$: Solution to $A^Ty=0$ and is also called as Left Null Spcae of $A$  The pictorial representation of these spaces with their dimension and basis is as follows:</description>
    </item>
    
    <item>
      <title>Matrix Independence, Span, Basis &amp; Dimension</title>
      <link>https://amitrajan012.github.io/post/chapter_7_independence_basis_and_dimension/</link>
      <pubDate>Sun, 30 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_7_independence_basis_and_dimension/</guid>
      <description>7.1 Independence Vectors $x_1,x_2, &amp;hellip;, x_n$ are linearly independent if no combinations of these vectors gives zero vector, except the zero combination. i.e.
$$\begin{align} c_1x_1 + c_2x_2 + &amp;hellip; + c_nx_n \neq 0; except \ \forall c_i=0 \end{align}$$
As we know that, for a $m \times n$ matrix $A$, if $m &amp;lt; n$, then we will have at least one non-zero solution to $Ax=0$. It implies that we will always find some non-zero combinations of $c_i$ that will make satisfy the above condition.</description>
    </item>
    
    <item>
      <title>Algorithm for solving $Ax=b$</title>
      <link>https://amitrajan012.github.io/post/chapter_6_solving_for_axb/</link>
      <pubDate>Tue, 25 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_6_solving_for_axb/</guid>
      <description>6.1 Algorithm for solving $Ax=b$ The idea behind this exercise is to come up with the solution for $Ax=b$. Let us start by taking a matrix $A$ and vector $b$.
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 \\ 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \end{bmatrix}; b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} \end{align}$$
The augumented matrix $\begin{bmatrix} A &amp;amp; b \end{bmatrix}$ can be formed and elimination steps can be performed on it as follows.</description>
    </item>
    
    <item>
      <title>Algorithm for solving $Ax=0$</title>
      <link>https://amitrajan012.github.io/post/chapter_5_computing_nullspace/</link>
      <pubDate>Thu, 20 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_5_computing_nullspace/</guid>
      <description>5.1 Algorithm for solving $Ax=0$ The idea behind this exercise is to come up with an algorithm to find the nullspace of a matrix $A$. Let us start by taking a matrix $A$.
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 \\ 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \end{bmatrix} \end{align}$$
One of the first thing to notice in $A$ is that $row_3$ is a linear combination of $row_1$ and $row_2$ ($row_3 = row_1 + row_2$).</description>
    </item>
    
    <item>
      <title>Vector Space and Subspace</title>
      <link>https://amitrajan012.github.io/post/chapter_4_vector_spaces/</link>
      <pubDate>Mon, 17 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_4_vector_spaces/</guid>
      <description>4.1 Vector Space and Subspace Vector Space is a set of vectors whose linear combinations belong to the same set. This means that whenever we pick $2$ vectors from a vector space and find thier linear combination, the resultant vector will be in the vector space. A vector space in two and three dimensions is $R^2$ and $R^3$.
We can make some common observations about vector spaces. Let $u,v$ be two vectors, then their linear combination can be described as $w=au+bv$ where $a,b$ are scalars.</description>
    </item>
    
    <item>
      <title>Inverse of a Matrix &amp; Factorization into $A=LU$</title>
      <link>https://amitrajan012.github.io/post/chapter_3_inverse_factorization_and_symmetric_matrices/</link>
      <pubDate>Wed, 12 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_3_inverse_factorization_and_symmetric_matrices/</guid>
      <description>3.1 Inverse of a Matrix &amp;amp; Factorization into $A=LU$ Given a square matrix $A$, $A^{-1}$ is it&amp;rsquo;s inverse if $AA^{-1} = A^{-1}A = I$, where $I$ is the Identity Matrix and we say that $A$ is invertible or nonsingular.
For a singular matrix $A$, the inverse does not exist and it&amp;rsquo;s determinant is $0$. Also, we can find a vector $x$ such that $Ax=0$. This means that one or more than one columns of $A$ is linear combination of other columns.</description>
    </item>
    
    <item>
      <title>Elimination &amp; Permutation with Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_2_elimination_and_permutation/</link>
      <pubDate>Sat, 08 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_2_elimination_and_permutation/</guid>
      <description>2.1 Elimination &amp;amp; Permutation with Matrices Elimination is the method which is used to solve a system of linear equations. Let $Ax=b$ is a system of linear equation where
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 1\\ 3 &amp;amp; 8 &amp;amp; 1\\ 0 &amp;amp; 4 &amp;amp; 1 \end{bmatrix}, b = \begin{bmatrix} 2 \\ 12 \\ 2 \end{bmatrix} \end{align}$$
The idea of elimination is to transform $A$ into a matrix where all the entries in the cell below the diagonal is 0.</description>
    </item>
    
    <item>
      <title>Geometry of Linear Equations &amp; Matrix Multiplications</title>
      <link>https://amitrajan012.github.io/post/chapter_1_geometry_of_linear_equations__matrix_multiplications/</link>
      <pubDate>Mon, 03 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_1_geometry_of_linear_equations__matrix_multiplications/</guid>
      <description>1.1 The Geometry of Linear Equations The fundamental goal of linear algebra is to solve a system of linear equations. Let us look at an example of a set of $2$ linear equations in $2$ unknowns:
$$\begin{align} 2x-y = 0 \\ -x+2y = 3 \end{align}$$
The above system of linear equation when written in matrix multiplication form can be represented as $Ax = b$ where $A$ is a $2 \times 2$ matrix, $x$ and $b$ are column vectors.</description>
    </item>
    
    <item>
      <title>The Wilcoxon Signed-Rank Test</title>
      <link>https://amitrajan012.github.io/post/the-wilcoxon-signed-rank-test/</link>
      <pubDate>Fri, 07 Dec 2018 07:26:51 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/the-wilcoxon-signed-rank-test/</guid>
      <description>The Wilcoxon Signed-Rank Test is a distribution free test or non-parametric test which is used to test the population mean when the samples do not come from any specific distribution. The idea behind The Wilcoxon Signed-Rank Test is to median-center the samples and give signed-ranks to the individual values. If the sum of the negative ranks (S-) is larger, we can conclude that the population mean is lesser than the mentioned value.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://amitrajan012.github.io/post/logistic-regression/</link>
      <pubDate>Wed, 05 Dec 2018 07:26:51 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/logistic-regression/</guid>
      <description>In a classification setting, logistic regression models the probability of a response $Y$ belonging to a particaular category. A simple linear regression can not be used for classification as the output of a linear regression can have a range that goes from $-\infty$ to $\infty$ (we need to find the values in the range [0, 1]). Instead, we can transform the output of linear regression such that the output is confined in the range [0, 1].</description>
    </item>
    
    <item>
      <title>Hypothesis Testing (Part 6)</title>
      <link>https://amitrajan012.github.io/post/detailed-hypothesis-testing_6/</link>
      <pubDate>Mon, 26 Nov 2018 07:26:51 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/detailed-hypothesis-testing_6/</guid>
      <description>### Tests for Variances of Normal Populations : Let $X_1, X_2, &amp;hellip;, X_n$ be a simple random sample from a normal population given as $N(\mu, \sigma^2)$. The sample variance $s^2$ is given as:
$$s^2 = \frac{1}{n-1}\sum_{i=1}^{n} (X_i - \overline{X})^2$$
Then the test statistic $\frac{(n-1)s^2}{\sigma_0^2}$ follows a chi-square distribution with $n-1$ degrees of freedom. The null hypothesis can take any of the form:
$$H_0: \sigma^2 \leq \sigma_0^2;\ \sigma^2 = \sigma_0^2;\ \sigma^2 \geq \sigma_0^2$$</description>
    </item>
    
    <item>
      <title>Hypothesis Testing (Part 5)</title>
      <link>https://amitrajan012.github.io/post/detailed-hypothesis-testing_5/</link>
      <pubDate>Sun, 25 Nov 2018 17:06:08 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/detailed-hypothesis-testing_5/</guid>
      <description>### Tests with Categorical Data : A generalization of the Bernoulli trial is the multinomial trial, which is an experiment that can rsult in any one of the $k$ outcomes where $k \geq 2$. Let the probabilities of the $k$ outcomes be denoted by $p_1, p_2, p_3, &amp;hellip;, p_k$ and the prespecified values be $p _{01}, p _{02}, &amp;hellip;, p _{0k}$. We need to conduct a test to chek whether the probabilities are equal to the prespecified values or not.</description>
    </item>
    
    <item>
      <title>Hypothesis Testing (Part 4)</title>
      <link>https://amitrajan012.github.io/post/detailed-hypothesis-testing_4/</link>
      <pubDate>Sun, 25 Nov 2018 11:36:18 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/detailed-hypothesis-testing_4/</guid>
      <description>### Distribution-Free Tests : The one assumption for the Student&amp;rsquo;s t tests performed above was the fact that the samples should come from the normal distribution. In distribution-free tests, this restriction is relaxed, i.e. the samnples are not required to come from any specific distribution. Distribution-free tests are sometimes called as nonparametric tests. Mainly, there are two types of distribution-free tests: Wilcoxon signed-rank test(test for population mean) and Wilcoxon rank-sum test / Mann-Whitney test (analogous to the two-sample t test).</description>
    </item>
    
    <item>
      <title>Hypothesis Testing (Part 3)</title>
      <link>https://amitrajan012.github.io/post/detailed-hypothesis-testing_3/</link>
      <pubDate>Sat, 24 Nov 2018 01:08:17 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/detailed-hypothesis-testing_3/</guid>
      <description>### Tests for the Difference Between Two Means (Large-Sample) : The basic idea to conduct the hypothesis test for difference between two means is to find the distribution for the difference of two means and test whether it is equal to 0 or not. Here is an example.
Example: Suppose that a production manager for a manufacturer of industrial machinery is concerned that ball bearings produced in environments with low ambient temperatures may have smaller diameters than those produced under higher temperatures.</description>
    </item>
    
    <item>
      <title>Hypothesis Testing (Part 2)</title>
      <link>https://amitrajan012.github.io/post/detailed-hypothesis-testing_2/</link>
      <pubDate>Fri, 23 Nov 2018 19:38:29 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/detailed-hypothesis-testing_2/</guid>
      <description>### Tests for a Population Proportion : The hypothesis testing for population proportion can be conducted in a similar manner. Here are some examples to depict it.
Example: A supplier of semiconductor wafers claims that of all the wafers he supplies, no more than 10% are defective. A sample of 400 wafers is tested, and 50 of them, or 12.5%, are defective. Can we conclude that the claim is false?</description>
    </item>
    
    <item>
      <title>Hypothesis Testing (Part 1)</title>
      <link>https://amitrajan012.github.io/post/detailed-hypothesis-testing_1/</link>
      <pubDate>Thu, 22 Nov 2018 09:17:41 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/detailed-hypothesis-testing_1/</guid>
      <description>Hypothesis Testing is a method by which we can test an assumption made for a population parameter. For example, the statement $\mu  10$ is an assumption or hypothesis about the population mean $\mu$. To check the validity of this hypothesis, we must conduct a hypothesis test. Hypothesis tests are closely related to confidence intervals. Prior to performing a hypothesis test, we need to formulate the null and alternate hypothesis.</description>
    </item>
    
    <item>
      <title>Confidence Intervals (Part 3)</title>
      <link>https://amitrajan012.github.io/post/confidence-intervals_3/</link>
      <pubDate>Tue, 20 Nov 2018 16:41:05 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/confidence-intervals_3/</guid>
      <description>#### Confidence Intervals with Paired Data : Sometimes an experiment is designed in such a way that each item in one sample is paired with an item in the other. Let $D_1, D-2, &amp;hellip;, D_n$ be a small random sample $(n \leq 30)$ of differences of the paired data. If the population of differences is approximately normal, the $100(1 - \alpha) %$ confidence interval for the mean difference $\mu_D$ is given as:</description>
    </item>
    
    <item>
      <title>Confidence Intervals (Part 2)</title>
      <link>https://amitrajan012.github.io/post/confidence-intervals_2/</link>
      <pubDate>Sun, 18 Nov 2018 05:07:15 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/confidence-intervals_2/</guid>
      <description>#### Confidence Intervals for Proportions : Let $X$ be the number of successes in $n$ independent Bernoulli trials with success probability $p$, where the number of trials $n$ is large enough, such that $X \sim Bin(n, p)$. Then the $100(1 - \alpha) %$ confidence interval for $p$ is:
$$\widehat{p} \pm z_{\alpha/2}\sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$$
where $\widehat{p}$ is the sample proportion and can be estimated as $\frac{X}{n}$. It should be noted that the quantity under the square root is the sample variance.</description>
    </item>
    
    <item>
      <title>Confidence Intervals (Part 1)</title>
      <link>https://amitrajan012.github.io/post/confidence-intervals_1/</link>
      <pubDate>Sat, 17 Nov 2018 11:27:09 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/confidence-intervals_1/</guid>
      <description>A confidence interval is a type of interval estimate, computed from the statistics of the observed data, that might contain the true value of an unknown population parameter. The interval has an associated confidence level that, loosely speaking, quantifies the level of confidence that the parameter lies in the interval.  #### Confidence Intervals for a Population Mean (Large-Sample) : For $X_1, X_2, &amp;hellip;, X_n$ be a large ($n &amp;gt; 30$) random sample from a population with mean $\mu$ and standard deviation $\sigma$, so that $\overline{X}$ is approximately normal (from Central Limit Theorem).</description>
    </item>
    
    <item>
      <title>Commonly used Distributions (Part 2)</title>
      <link>https://amitrajan012.github.io/post/commonly-used-distributions_2/</link>
      <pubDate>Fri, 16 Nov 2018 01:00:00 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/commonly-used-distributions_2/</guid>
      <description>#### The Normal Distribution : The normal distribution is a continuous distribution with any mean and a positive variance. The probability density function of a normal distribution with mean $\mu$ and variance $\sigma^2$ is given as:
$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$$
The above normal distribution is denoted as $X \sim N(\mu, \sigma^2)$ with mean and variance as:
$$\mu_X = \mu$$
$$\sigma_X^2 = \sigma^2$$
For a normal distribution, about 68% of the population is in the interval $\mu \pm \sigma$, 95% in $\mu \pm 2\sigma$ and 99.</description>
    </item>
    
    <item>
      <title>Commonly used Distributions (Part 1)</title>
      <link>https://amitrajan012.github.io/post/commonly-used-distributions_1/</link>
      <pubDate>Thu, 15 Nov 2018 12:03:41 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/commonly-used-distributions_1/</guid>
      <description>#### The Bernoulli Distribution : Bernoulli trial is an experiment that can result in two outcomes: success (with probability $p$) and failure (with probability $1-p$). A Bernoulli random variable $X$ can be represented as $X \sim Bernoulli(p)$. It&amp;rsquo;s mean $\mu_X$ and variance $\sigma_X^2$ can be computed as:
$$\mu_X = 0 \times (1-p) + 1 \times p = p$$
$$\sigma_X^2 = (0-p)^2(1-p) + (1-p)^2p = p(1-p)$$
 #### The Binomial Distribution : When a set of $n$ independent Bernoulli trials are conducted, each with a success probability of $p$, a random variable $X$ which is equal to the number of success in these trials is said to have the binomial distribution with parameters $n$ and $p$ and is represented as $X \sim Bin(n, p)$.</description>
    </item>
    
    <item>
      <title>Measurement and Propagation of Error (Part 2)</title>
      <link>https://amitrajan012.github.io/post/measurement-and-propagation-of-error_2/</link>
      <pubDate>Wed, 14 Nov 2018 03:08:14 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/measurement-and-propagation-of-error_2/</guid>
      <description>#### Linear Combinations of Dependent Measurements : In the case of dependent measurement, to quantify the uncertainty, we need to know the value of covariance for all the possible pairs of measurements. This is practically not feasible. In this case, an upper bound can be placed on the uncertainty. If $X_1, X_2, &amp;hellip;, X_n$ are $n$ dependent measurements and $c_1, c_2, &amp;hellip;, c_n$ are constants, then the uncertainty of $c_1X_1 + c_2X_2 + &amp;hellip; + c_nX_n$ can be bounded as:</description>
    </item>
    
    <item>
      <title>Measurement and Propagation of Error (Part 1)</title>
      <link>https://amitrajan012.github.io/post/measurement-and-propagation-of-error/</link>
      <pubDate>Wed, 14 Nov 2018 02:18:29 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/measurement-and-propagation-of-error/</guid>
      <description>Any measurement in a scientific or an engineering process, consists of two error parts: systematic error or bias and random error. The bias is the part of the error that is same for every measurement. Random error varies from measurement to measurement and averages out to 0 in the long run. Hence, the measured value can be written as: $$Measured \ Value = True \ Value + Bias + Random \ Error$$</description>
    </item>
    
    <item>
      <title>Random Variables (Part 3: Jointly Distributed Random Variables)</title>
      <link>https://amitrajan012.github.io/post/randomvariables_part3/</link>
      <pubDate>Tue, 13 Nov 2018 06:11:47 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/randomvariables_part3/</guid>
      <description>#### Independent Random Variables : If $X$ and $Y$ are independent random variables and $S$ and $T$ are sets of numbers then,
$$P(X \in S \ and \ Y \in T) = P(X \in S) P(Y \in T)$$
Hence, for independent random variables $X_1, X_2, &amp;hellip;, X_n$ and constants $c_1, c_2, &amp;hellip;, c_n$, the variance of linear combination $c_1X_1 + c_2X_2 + &amp;hellip; + c_nX_n$ is given as:
$$\sigma _{c_1X_1 + c_2X_2 + &amp;hellip; + c_nX_n}^2 = c_1^2\sigma _{X_1}^2 + c_2^2\sigma _{X_2}^2 + &amp;hellip; + c_n^2\sigma _{X_n}^2$$</description>
    </item>
    
    <item>
      <title>Random Variables (Part 2: Continuous Random Variables)</title>
      <link>https://amitrajan012.github.io/post/randomvariables_part2/</link>
      <pubDate>Mon, 12 Nov 2018 03:01:07 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/randomvariables_part2/</guid>
      <description>#### Continuous Random Variables : A continuous random variable is a random variable which can take infinitely many values. The probabilities associated with a continuous RV is defined by probability density function(PDF).
 #### Probability Density Function (PDF) : As a continuous RV takes infinite values, the probability $P(X=x)$ for it can not be defined and takes a value of 0. Instead we define a probability density funaction, which intutively depicts probability per unit space, where space is defined by the range of the underlying random variable.</description>
    </item>
    
    <item>
      <title>Random Variables (Part 1: Discrete Random Variables)</title>
      <link>https://amitrajan012.github.io/post/random-variables/</link>
      <pubDate>Sun, 11 Nov 2018 13:11:27 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/random-variables/</guid>
      <description>Random Variable is a variable whose possible values are numerical outcomes of a random phenomenon or experiment. In other words, a random variable assigns a numeric value to each outcome of a random experiment. For example, if we roll a fair die, the random variable $X$ describing the experiment will take the values $1,2,3,4,5,6$ (which are the possible outcomes of the experiment). There are two types of random variables: discrete and continuous.</description>
    </item>
    
    <item>
      <title>Performance Metrics for Classification Algorithms</title>
      <link>https://amitrajan012.github.io/post/performance-metrics-for-classification-algorithms/</link>
      <pubDate>Mon, 29 Oct 2018 12:11:37 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/performance-metrics-for-classification-algorithms/</guid>
      <description>There are several metrics that can be used to measure the performance of a classification algorithm. The choice for the same depends on the problem statement and serves an important role in model selection.
 ### Confusion Matrix : Confusion matrix is one of the easiest and the most intutive way to find the correctness and accuracy of the model. It serves as the building block for all the other performance measures.</description>
    </item>
    
    <item>
      <title>Hypothesis testing</title>
      <link>https://amitrajan012.github.io/post/hypothesis-testing/</link>
      <pubDate>Mon, 29 Oct 2018 02:01:07 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/hypothesis-testing/</guid>
      <description>### Introduction : Hypothesis testing is a procedure that is used to determine that whether a made statistical statement (known as hypothesis) is a reasonable one and should not be rejected, or is unreasonable and should be rejected. Hypothesis testing setup is initialized by formulating a Null Hypothesis ($H_0$), which is the hypothesis associated with a contradiction to the theory that one would like to prove and Alternate Hypothesis ($H_A$), which is the hypothesis associated with the theory that one would like to prove.</description>
    </item>
    
    <item>
      <title>Maximum Likelihood Estimation</title>
      <link>https://amitrajan012.github.io/post/maximum-likelihood-estimation-/</link>
      <pubDate>Sun, 28 Oct 2018 17:11:47 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/maximum-likelihood-estimation-/</guid>
      <description>### Introduction : Maximum Likelihood Estimation is the method of estimating the parameters of a statistical model, given the observations. It attempts to find the parameter values that maximize the likelihood function. The process can be viewed as finding the parameters that maximize the likelihood of getting the data we observed for a particular set of statistical models.
Suppose we have the data points (random samples) $X_1, X_2, &amp;hellip;, X_n$ which belong to a distribution which depends on one or more unknown parameters $\theta_1, \theta_2, &amp;hellip;, \theta_m$ with probability density (or mass) function $f(x_i; \theta_1, \theta_2, &amp;hellip;, \theta_m)$.</description>
    </item>
    
    <item>
      <title>Naive Bayes Classifier</title>
      <link>https://amitrajan012.github.io/post/naive-bayes-classifier/</link>
      <pubDate>Wed, 24 Oct 2018 05:01:19 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/naive-bayes-classifier/</guid>
      <description>Introduction : Naive Bayes is an extremely fast classification algorithm which uses Bayes Theorem as its basic building block. It assumes that the features or predictors in the dataset are independent.
The Bayes theorem is given as:
$$P(c|X) = \frac{P(c)P(X|c)}{P(X)}$$
where $c$ denotes a class label and $X$ is the predictor. The probabilities $P(c)$ and $P(X)$ are the prior probabilities of the class and the predictor. $P(X|c)$ is the prior probability or likelihood of observing a feature $X$ given class $c$.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 9</title>
      <link>https://amitrajan012.github.io/post/chapter-9-correlation/</link>
      <pubDate>Mon, 10 Sep 2018 11:13:31 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-9-correlation/</guid>
      <description>9.1 Standard scores The main challenge in measuring correlation is that the variables we want to compare might not be expressed in the same units. There are two common solutions to this problem:
 Transform all values to standard scores. This leads to Pearson coefficient of correlation. Transform all values to their percentile ranks. This leads to Spearman coefficient.  Normalizing the score means subtracting mean from every value and dividing it by standard deviation.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 8</title>
      <link>https://amitrajan012.github.io/post/chapter-8-estimation/</link>
      <pubDate>Tue, 04 Sep 2018 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-8-estimation/</guid>
      <description>8.1 The estimation game In a rudimentary way, sample mean can be used to estimate a distribution. The process is called estimation and the statistic we used (sample mean) is called an estimator. If there are no outliers, sample mean minimizes the mean squared error (MSE). A maximum likelihood estimator (MLE) is an estimator that has the highest chance of being right (value with highest probability).
 Exercise 8.1: Write a function that draws 6 values froma normal distribution with $\mu$ = 0 and $\sigma$ = 1.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 7</title>
      <link>https://amitrajan012.github.io/post/chapter-7-hypothesis-testing/</link>
      <pubDate>Thu, 30 Aug 2018 06:17:39 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-7-hypothesis-testing/</guid>
      <description>The process of Hypothesis Testing can be summarized by following three steps:   Null Hypothesis: The null hypothesis is a mode of the system based on the assumption that apparent effect was actually due to chance.
  p-value: The p-value is the probability of the apparent effect under the null hypothesis.
  Interpretation: Based on the p-value, we can conclude that whether the effect is statistically significant or not.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 6</title>
      <link>https://amitrajan012.github.io/post/chapter-6-operations-on-distributions/</link>
      <pubDate>Sun, 26 Aug 2018 16:27:09 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-6-operations-on-distributions/</guid>
      <description>6.1 Skewness Skewness is a statistic that measures the assymetry of a distribution. Sample skewness is defined as:
$$Skewness = \frac{Mean\ Cubed\ Deviation}{Mean\ Squared\ Deviation^{\frac{3}{2}}}$$
Negative skewness means that the distribution skews left and positive skewness means that the distribution is skewed to right. Outliers have a disproportionate effect on skewness.
Another way to evaluate the asymmetry of a distribution is to compare mean and median. For distributions that skews left, mean is less than the median.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 5</title>
      <link>https://amitrajan012.github.io/post/chapter-5-probability/</link>
      <pubDate>Wed, 22 Aug 2018 09:12:36 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-5-probability/</guid>
      <description>The belief in which probability is expressed in terms of frequencies is called as frequentism. Frequentists believe that if there is no set of identical trials, there is no probability.
An alternative is Bayesianism, which defines probability as a degree of belief that an event will occur. By this definition, the notion of probability can be applied in almost any cicumstance. One drawback of Bayesian probability is that it depends on person&amp;rsquo;s state of knowledge.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 4</title>
      <link>https://amitrajan012.github.io/post/chapter-4-continuous-distributions/</link>
      <pubDate>Fri, 17 Aug 2018 19:02:16 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-4-continuous-distributions/</guid>
      <description>4.1 The exponential distribution CDF of the exponential distribution is defined as (where lambda determines the shape of the distribution): $$CDF(x) = 1-e^{-\lambda x}$$ Mean and Median of exponential distribution is computed as follows: $$Mean = \frac{1}{\lambda}$$ $$Median = \frac{ln(2)}{\lambda}$$ In the real world, exponential distribution is encountered when we look at the series of events and measure the time between them which is called as interval times. The exponential distribution for different values of lambda is shown below.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 3</title>
      <link>https://amitrajan012.github.io/post/chapter-3-cumulative-distribution-functions/</link>
      <pubDate>Sun, 12 Aug 2018 09:22:46 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-3-cumulative-distribution-functions/</guid>
      <description>3.1 The class size paradox For a probability distribution, the mean calculated from its PMF is lower than the one calculated by taking a sample from it. This happens because the larger classes tend to get oversampled. Exercise 3.1 Build the PMF of the college class-size data and compute the mean as perceived by the Dean. Find the distribution of class sizes as perceived by students and compute its mean.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 2</title>
      <link>https://amitrajan012.github.io/post/chapter-2-descriptive-statistics/</link>
      <pubDate>Wed, 08 Aug 2018 04:17:12 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-2-descriptive-statistics/</guid>
      <description>2.1 Means and averages Mean of a sample is a summary statistics that can be computed as (where n is the total number of samples): $$\mu = \frac{1}{n} \sum_i{x_i}$$ An average is one of many summary statistics that can be used to describe the typical value or the central tendency of a sample.
2.2 Variance As mean describes the central tendency of a sample, Variance is intended to describe the spread.</description>
    </item>
    
    <item>
      <title>Think Stats: Chapter 1</title>
      <link>https://amitrajan012.github.io/post/chapter-1-statistical-thinking-for-programmers/</link>
      <pubDate>Sun, 05 Aug 2018 10:02:08 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-1-statistical-thinking-for-programmers/</guid>
      <description>1.1 Do first babies arrive late? Anecdotal Evidence  is based on data that is unpublished and usually personal. For example, &amp;ldquo;My two friends that have given birth recently to their first babies, BOTH went almost 2 weeks overdue before going into labour or being induced.  Anecdotal Evidence usually fail because of Small number of observations, Selection bias (People who join a discussion of this question might be interested because their first babies were late.</description>
    </item>
    
    <item>
      <title>Content Based Movie Recommendation Engine</title>
      <link>https://amitrajan012.github.io/post/introductory-data-analysis-and-recommendation-engine/</link>
      <pubDate>Mon, 30 Jul 2018 02:12:38 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/introductory-data-analysis-and-recommendation-engine/</guid>
      <description>Introduction The dataset consists information of 5000 movies with their genres, budget, revenue, production company, revenue, user-rating , vote-count and popularity as the primary fields. It also has the detailed information of cast and crew.
 # import modules import pandas as pd import numpy as np from matplotlib import pyplot as plt import json import cufflinks as cf import seaborn as sns import plotly.graph_objs as go import plotly from wordcloud import WordCloud plotly.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 6: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part6/</link>
      <pubDate>Thu, 19 Jul 2018 06:25:52 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part6/</guid>
      <description>Applied Q7. In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let $r_{ij}$ denote the correlation between the ith and jth observations, then the quantity $1r _{ij}$ is proportional to the squared Euclidean distance between the ith and jth observations.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 5: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part5/</link>
      <pubDate>Sun, 15 Jul 2018 03:51:27 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part5/</guid>
      <description>10.7 Exercises Conceptual Q1. This problem involves the K-means clustering algorithm.
(a) Prove (10.12).
Sol: Equation 10.12 is:
$$\frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} (x _{ij} - x _{i^{&amp;rsquo;}j})^2 = 2\sum _{i \in C_k} \sum _{j=1}^{p} (x _{ij} - \bar{x} _{kj})^2$$
where $\bar{x} _{kj} = \frac{1}{|C_k|} \sum _{i \in C_k} x _{ij}$, is the mean of feature $j$ in cluster $C_k$. Expanding LHS, we get
$$\frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} (x _{ij} - x _{i^{&amp;rsquo;}j})^2 = \frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{ij}^2 + \frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{i^{&amp;rsquo;}j}^2 - \frac{2}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{ij}x _{i^{&amp;rsquo;}j} \ = 2 \sum _{i \in C_k} \sum _{j=1}^{p} x _{ij}^2 - \frac{2}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{ij}x _{i^{&amp;rsquo;}j} $$</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 4: Clustering Methods, Hierarchical Clustering)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part4/</link>
      <pubDate>Thu, 12 Jul 2018 13:01:01 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part4/</guid>
      <description>10.3.2 Hierarchical Clustering K-means clustering has a disadvantage that there is a need to pre-specify the number of clusters $K$. Hierarchical clutsring is an alternative approach which is free from this problem which results in an altarnative tree-based representation of the observations, called as dendrogram.
The most common technique used for hierarchical clustering is bottom-up or agglomerative clustering. It is based on the fact that the dendrogram (generally depicted as an upside-down tree) is built starting from leaves and combining the clusters up to the trunk.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 3: Clustering Methods, K-Means Clustering)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part3/</link>
      <pubDate>Mon, 09 Jul 2018 17:09:41 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part3/</guid>
      <description>10.3 Clustering Methods Clustering is a technique for finding subgroups or clusters in a data set based on similarity between individual observations. For clustering, we need to define the measure of similarity which depends on the knowledge of the data set. Two best known clustering methods are K-means clustering and hierarchical clustering. In K-means clustering, we partition the observations into a pre-defined number of clusters. In hierarchical clustering, the number of clusters is unknown and the results of clustering is represented as a dendrogram, which is a tree-like visualization technique that allows us to view the clustering results for various number of clusters (from 1 to $n$).</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 2: More on PCA)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part2/</link>
      <pubDate>Fri, 06 Jul 2018 01:19:52 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part2/</guid>
      <description>10.2.3 More on PCA Scaling the Variables The results of PCA also depend on the fact that whether the variables are individually scaled or not. If we perform PCA on the unscaled variables, the variables with higher variance will have very large loading. As it is undesirable for the principal components obtained to depend on the scale of the variables, we scale each variables to have the standard deviation 1 before performing PCA.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 1: Principal Components Analysis)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part1/</link>
      <pubDate>Wed, 04 Jul 2018 03:09:02 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part1/</guid>
      <description>Unsupervised Learning 10.1 The Challenge of Unsupervised Learning Unsupervised learning is often performed as a a part of an exploratory data analysis. In unsupervised learning there is no practical way to assess the performance of the model as we do not have the true answer.
10.2 Principal Components Analysis In the case of a large set of correlated variables, principal component analysis helps in summarizing the data set with a small number of representative variables that explains most of the variability in the data.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 9: Support Vector Machines (Part 5: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/support-vector-machines_part5/</link>
      <pubDate>Fri, 29 Jun 2018 06:19:12 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/support-vector-machines_part5/</guid>
      <description>Applied Q4. Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 9: Support Vector Machines (Part 4: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/support-vector-machines_part4/</link>
      <pubDate>Tue, 26 Jun 2018 03:29:17 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/support-vector-machines_part4/</guid>
      <description>9.7 Exercises Conceptual Q1. This problem involves hyperplanes in two dimensions.
(a) Sketch the hyperplane 1 + 3X1  X2 = 0. Indicate the set of points for which 1 + 3X1  X2 &amp;gt; 0, as well as the set of points for which 1 + 3X1  X2 &amp;lt; 0.
(b) On the same plot, sketch the hyperplane 2 + X1 + 2X2 = 0. Indicate the set of points for which 2+ X1 +2X2 &amp;gt; 0, as well as the set of points for which 2+ X1 + 2X2 &amp;lt; 0.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 9: Support Vector Machines (Part 3: Support Vector Machines)</title>
      <link>https://amitrajan012.github.io/post/support-vector-machines_part3/</link>
      <pubDate>Sat, 23 Jun 2018 04:19:37 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/support-vector-machines_part3/</guid>
      <description>9.3 Support Vector Machines 9.3.1 Classification with Non-linear Decision Boundaries Support vector classifiers, which are designed to work in the setting of linear decision boundary, can be extended to handle the case of non-linear decision boundary by enlarging the feature space using polynomial transformation of the predictors. For example, if we have a $p$-dimensional feature space given as: $X_1, X_2, &amp;hellip;, X_p$, we could instead fit a support vector classifier using $2p$ features: $X_1, X_1^2, X_2, X_2^2, &amp;hellip;, X_p, X_p^2$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 9: Support Vector Machines (Part 2: Support Vector Classifiers)</title>
      <link>https://amitrajan012.github.io/post/support-vector-machines_part2/</link>
      <pubDate>Wed, 20 Jun 2018 02:24:24 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/support-vector-machines_part2/</guid>
      <description>9.2 Support Vector Classifiers 9.2.1 Overview of the Support Vector Classifier The maximal margin classifiers can be sensitive to individual observations. Sometimes, adding a single observation in the data set, can lead to dramatic change in the separating hyperplane. The sensitivity and the low margin for a maximal margin classifier may suggest that the maximal margin classifier has overfit the training data. So, sometimes we may be willing to consider a classifier based on a hyperplane that does not perfectly separate the two classes, and hence, will be more robust (or less sensitive to individual observations) and will give better results for the unseen data points.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 9: Support Vector Machines (Part 1: Maximal Margin Classifier)</title>
      <link>https://amitrajan012.github.io/post/support-vector-machines_part1/</link>
      <pubDate>Tue, 19 Jun 2018 12:14:23 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/support-vector-machines_part1/</guid>
      <description>Support Vector Machines Support vector machine is a generalization of a simple and intutive classifier called the maximal margin classifier. Maximal margin classifier has a limitation that it can be only applied to a data set whose classes are seperated by a linaer boundary.
9.1 Maximal Margin Classifier 9.1.1 What Is a Hyperplane? In a $p$-dimensional space, a hyperplane is a flat affine subspace of $p-1$ dimensions. In two dimensions, the hyperplane is a line and can be given as:</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 4: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part4/</link>
      <pubDate>Sat, 16 Jun 2018 01:34:53 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part4/</guid>
      <description>Applied Q 7. In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.
import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error from sklearn.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part3/</link>
      <pubDate>Thu, 14 Jun 2018 07:14:13 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part3/</guid>
      <description>8.4 Exercises Conceptual Q2. It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form
$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$
Explain why this is the case. You can begin with (8.12) in Algorithm 8.2.
Sol: As for depth-one trees, value of $d$ is 1. Each tree is generated by splitting the data on only one predictor and the final model is formed by adding the shrunken version of them repeatedly.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 2: Bagging, Random Forests, Boosting)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part2/</link>
      <pubDate>Wed, 13 Jun 2018 13:04:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part2/</guid>
      <description>8.2 Bagging, Random Forests, Boosting 8.2.1 Bagging The decision trees discussed above suffers from a problem of high variance. Bootstrap aggregation or bagging is a procedure that reduces the variance of a statistical learning method.
Give a set of $n$ independent observation sets $Z_1, Z_2, &amp;hellip;, Z_n$, each with variance $\sigma^2$, the variance of the mean $\bar{Z}$ is given by $\sigma^2/n$, i.e. averaging a set of observations reduces variance. Hence, a natural way to reduce the variance of a statistical model is to take many training samples from the population, fit individual models on them, and give the average of them as the final model.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 1: Decision Trees)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part1/</link>
      <pubDate>Mon, 11 Jun 2018 17:07:13 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part1/</guid>
      <description>Tree-Based Methods Tree-based methods stratify or segment the predictor space into a number of simple regions. To make a prediction, we use the mean or the mode of the training observations in the region in which the observation to be predicted belongs. The set of splitting rules can be summarized via a tree, these methods are also known as decision tree methods. Bagging, random forests and boosting produce multiple trees and then combine them in a single model to make the prediction.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 6: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part6/</link>
      <pubDate>Fri, 08 Jun 2018 03:17:36 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part6/</guid>
      <description>Applied Q6. In this exercise, you will further analyze the Wage data set considered throughout this chapter.
(a) Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree d for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data.
Sol: The optimal degree of polynomial selected from cross-validation is 4.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 5: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part5/</link>
      <pubDate>Wed, 06 Jun 2018 06:27:22 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part5/</guid>
      <description>7.9 Exercises Conceptual Q1. It was mentioned in the chapter that a cubic regression spline with one knot at  can be obtained using a basis of the form $x, x^2, x^3, (x  )^3 _+$, where $(x  )^3 _+ = (x  )^3$ if x &amp;gt;  and equals 0 otherwise. We will now show that a function of the form $f(x) = _0 + _1x + _2x^2 + _3x^3 + _4(x  )^3 _+$ is indeed a cubic regression spline, regardless of the values of $_0, _1, _2, _3, _4$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 4: Local Regression, Generalized Additive Models)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part4/</link>
      <pubDate>Mon, 04 Jun 2018 16:12:19 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part4/</guid>
      <description>7.6 Local Regression Local regression comutes the fit at a target point $x_0$ using only the nearby training observstions. The algorithm for local regression is as follows:
 Gather the $k$ points closest to $x_0$. Assign a weight $K_{i0} = K(x_i, x_0)$ to all the points in the neighborhood such that the points that are farthest have lower weights. All the points except from these $k$ nearest neighbors have weigth 0.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 3: Smoothing Splines)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part3/</link>
      <pubDate>Wed, 30 May 2018 06:02:06 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part3/</guid>
      <description>7.5 Smoothing Splines 7.5.1 An Overview of Smoothing Splines Regression splines are created by specifying a set of knots, producing a sequence of basis functions and then estimate spline coefficients using least squares.
To fit a smooth curve to a data set, we need to find a function $g(x)$ such that $RSS = \sum_{i=1}^{n}(y_i - g(x_i))^2$ is minimum. If we do not put any constraint on $g(x)$, we can always find a function $g(x)$, which will make RSS 0.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 2: Regression Splines)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part2/</link>
      <pubDate>Mon, 28 May 2018 11:12:18 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part2/</guid>
      <description>7.4 Regression Splines Regression splines are flixible class of basis functions that extend upon polynomial and piecewise constant regression approaches.
7.4.1 Piecewise Polynomials Piecewise polynomial regression fits separate low-degree polynomials over different regions of $X$. For example, a piecewise squared polynomial fits squared regression model of the form
$$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i$$
where the coefficients $\beta_0, \beta_1, \beta_2$ differs in different parts of the range of $X$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 1: Polynomial Regression, Step Functions, Basis Functions)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part1/</link>
      <pubDate>Mon, 28 May 2018 04:22:38 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part1/</guid>
      <description>Moving Beyond Linearity Lineaer models have its limitations in terms of predictive power. Linear models can be extended simply as:
  Polynomial regression extends linear regression by adding extra higher order predictors (predictors rasied to higher order powers).
  Step functions cut the range of a variable into $K$ distinct regions in order to produce a qualitative variable.
  Regression splines is the extension of polynomial regression and step functions.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 6: Linear Model Selection and Regularization (Part 5: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part5/</link>
      <pubDate>Sat, 26 May 2018 11:08:14 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part5/</guid>
      <description>Applied Q8. In this exercise, we will generate simulated data, and will then use this data to perform best subset selection.
(a) Use the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector  of length n = 100.
import numpy as np  np.random.seed(5) X = np.random.normal(0, 1, 100) e = np.random.normal(0, 1, 100) (b) Generate a response vector Y of length n = 100 according to the model $Y = _0 + _1X + _2X^2 + _3X^3 + \epsilon$, where $_0, _1, _2,$ and $_3$ are constants of your choice.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 6: Linear Model Selection and Regularization (Part 4: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part4/</link>
      <pubDate>Fri, 25 May 2018 06:18:38 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part4/</guid>
      <description>6.8 Exercises Conceptual Q1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:
(a) Which of the three models with k predictors has the smallest training RSS?
Sol: Training RSS is minimum for best subset selection.
(b) Which of the three models with k predictors has the smallest test RSS?</description>
    </item>
    
    <item>
      <title>ISLR Chapter 6: Linear Model Selection and Regularization (Part 3: Dimension Reduction Methods)</title>
      <link>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part3/</link>
      <pubDate>Thu, 24 May 2018 01:06:16 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part3/</guid>
      <description>6.3 Dimension Reduction Methods Instead of performing a least squares regression on all the $p$ predictors, we can transform the predictors and then fit a least squares model on the transformed variables. Let the transformed variables be $Z_1, Z_2, &amp;hellip;, Z_M$, where $M &amp;lt; p$, where each of $Z_m$s is a linear combination of predictors $X_1, X_2, &amp;hellip;, X_p$.i.e.
$$Z_m = \sum _{j=1}^{p} \phi _{jm}X_j$$
We can then fit the least squares regression model as $Z_m$s as the predictors:</description>
    </item>
    
    <item>
      <title>ISLR Chapter 6: Linear Model Selection and Regularization (Part 2: Shrinkage Methods)</title>
      <link>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part2/</link>
      <pubDate>Tue, 22 May 2018 21:16:20 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part2/</guid>
      <description>6.2 Shrinkage Methods As an alternative to subset selection methods, a model containing all the $p$ predictors can be fit using a technique that constrains or regularizes the coefficient estimates (or shrinks the coefficeint estimates towards 0). Two best known techniques for shrinking the coefficient estimates towards 0 are: ridge regression and the lasso.
6.2.1 Ridge Regression In a least squares fitting, the parameters of the model is estimated by minimizing</description>
    </item>
    
    <item>
      <title>ISLR Chapter 6: Linear Model Selection and Regularization (Part 1: Subset Selection)</title>
      <link>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part1/</link>
      <pubDate>Mon, 21 May 2018 13:08:40 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part1/</guid>
      <description>Linear Model Selection and Regularization Befor moving to non-linear models, there are certain other fitting procedures through which a plain linear model can be improved. These alternate fitting procedures can yield better prediction accuracy and model interpretability.
  Prediction Accuracy: Provided that the relationship between predictors and response is linear, the least square estimates will have low bias. If n &amp;raquo; p, this model will have low variance as well.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 5: Resampling Methods (Part 4: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/resampling-methods_part4/</link>
      <pubDate>Sat, 19 May 2018 13:08:40 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/resampling-methods_part4/</guid>
      <description>Applied Q5. In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.
import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np import statsmodels.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 5: Resampling Methods (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/resampling-methods_part3/</link>
      <pubDate>Fri, 18 May 2018 07:18:30 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/resampling-methods_part3/</guid>
      <description>5.4 Exercises Conceptual Q1. Using basic statistical properties of the variance, as well as singlevariable calculus, derive that the value of $\alpha$ which minimizes $Var(\alpha X + (1 - \alpha) Y)$ is:
$$\alpha = \frac{\sigma_Y^2 - \sigma _{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma _{XY}}$$
Sol: As we know that $Var(aX + bY) = a^2 Var(X) + b^2 Var(Y) + 2abCov(X, Y)$, the above quantity (that needs to be minimized) can be transformed as:</description>
    </item>
    
    <item>
      <title>ISLR Chapter 5: Resampling Methods (Part 2: The Bootstrap)</title>
      <link>https://amitrajan012.github.io/post/resampling-methods_part2/</link>
      <pubDate>Fri, 18 May 2018 02:26:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/resampling-methods_part2/</guid>
      <description>5.2 The Bootstrap Bootstrap can be used to to quantify the uncertainty associated with a given statistical model. For example, bootstrap can be used to estimate standard errors (which measures the uncertainty) of the coefficients from a linear regression fit. Bootstrap can be applied to a wide range of statistical learning methods. The method of bootstrap is explained below via an example:
Suppose we wish to invest money in two financial assests which yield returns of $X$ and $Y$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 5: Resampling Methods (Part 1: Cross-Validation)</title>
      <link>https://amitrajan012.github.io/post/resampling-methods_part1/</link>
      <pubDate>Thu, 17 May 2018 05:22:19 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/resampling-methods_part1/</guid>
      <description>Resampling Methods Resampling methods involve repeatedly drawing samples from training data and refitting a model on them. Resampling approaches can be computationally expensive. Cross-validation and bootstrap are two of the most commonly used resampling methods. Cross-validation can be used to estimate the test error rate associated with a given model in order to evaluate its performance. The process of evaluating a model&amp;rsquo;s performance is called model assessment. The process of selecting proper level of flexibility for a model is known as model selection.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 4: Classification (Part 4: Exercises- Applied)</title>
      <link>https://amitrajan012.github.io/post/classification_part4/</link>
      <pubDate>Wed, 16 May 2018 10:20:15 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/classification_part4/</guid>
      <description>Applied Q10. This question should be answered using the Weekly data set.
(a) Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?
import seaborn as sns weekly = pd.read_csv(&amp;#34;data/Weekly.csv&amp;#34;)  sns.pairplot(weekly, vars=[&amp;#39;Lag1&amp;#39;, &amp;#39;Lag2&amp;#39;, &amp;#39;Lag3&amp;#39;, &amp;#39;Lag4&amp;#39;, &amp;#39;Lag5&amp;#39;, &amp;#39;Volume&amp;#39;], hue=&amp;#39;Direction&amp;#39;) (b) Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 4: Classification (Part 3: Exercises- Conceptual)</title>
      <link>https://amitrajan012.github.io/post/classification_part3/</link>
      <pubDate>Tue, 15 May 2018 09:02:08 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/classification_part3/</guid>
      <description>4.7 Exercises Conceptual Q1. Using a little bit of algebra, prove that the logistic function representation and logit representation for the logistic regression model are equivalent.
Sol:  Logistic function representation is given as:
$$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$$
then $1-p(X) = \frac{1}{1 + e^{\beta_0 + \beta_1X}}$, Taking the ratio of these two and then taking the log, we get
$$log\bigg( \frac{p(X)}{1-p(X)} \bigg) = \beta_0 + \beta_1X$$</description>
    </item>
    
    <item>
      <title>ISLR Chapter 4: Classification (Part 2: Linear Discriminant Analysis)</title>
      <link>https://amitrajan012.github.io/post/classification_part2/</link>
      <pubDate>Mon, 14 May 2018 11:12:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/classification_part2/</guid>
      <description>4.4 Linear Discriminant Analysis In logistic regression, we model the the conditional distribution of response $Y$ given the predictors $X$. As an alternative approach, we model the distribution of predictors X seperately for each of the response classe. We then use Bayes&amp;rsquo; Theorem to flip these around into estimates for Pr(Y = k | X = x). When these distributions are assumed to be normal, this model is very similar to logistic regression.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 4: Classification (Part 1: Logistic Regression)</title>
      <link>https://amitrajan012.github.io/post/classification_part1/</link>
      <pubDate>Sun, 13 May 2018 02:17:58 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/classification_part1/</guid>
      <description>Classification A process for predicting qualitative or categorical variables is called as Classification.
4.1 An Overview of Classification The dataset used in this chapter will be Default dataset. We will predict that whether an individual will default on his/her credit card payment on the basis of annual income and monthly credit card balance. The data is displayed below:
import pandas as pd import seaborn as sns import matplotlib.pyplot as plt  default = pd.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 3: Linear Regression (Part 5: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/linear-regression_part5/</link>
      <pubDate>Fri, 11 May 2018 05:18:18 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-regression_part5/</guid>
      <description>Applied Solution 8:
(a) Perform linear regression on auto data with mpg as response and horsepower as the predictor and display the summary results.
import statsmodels.api as sm  X = sm.add_constant(auto[[&amp;#39;horsepower&amp;#39;]], prepend=True) model = sm.OLS(auto[&amp;#39;mpg&amp;#39;], X) result = model.fit() print(result.summary()) print(&amp;#34;Prediction for horsepower 98: &amp;#34; +str(result.predict([1, 98]))) print(&amp;#34;95% CI: &amp;#34; +str(result.conf_int(alpha=0.05, cols=None)))  OLS Regression Results ============================================================================== Dep. Variable: mpg R-squared: 0.606 Model: OLS Adj. R-squared: 0.605 Method: Least Squares F-statistic: 599.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 3: Linear Regression (Part 4: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/linear-regression_part4/</link>
      <pubDate>Thu, 10 May 2018 10:28:18 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-regression_part4/</guid>
      <description>3.7 Exercises Conceptual Solution: The linear fit can be given as:
$$50 + (20 \times GPA) + (0.07 \times IQ) + (35 \times GENDER) + (0.01 \times GPA \times IQ) - (10 \times GPA \times GENDER)$$
(a) For a fixed value of IQ and GPA, the average salary for male will be $50 + 20 \times GPA$ and for the female it will be $85 + 20 \times GPA - 10 \times GPA$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 3: Linear Regression (Part 3: Other Considerations in the Regression Model)</title>
      <link>https://amitrajan012.github.io/post/linear-regression_part3/</link>
      <pubDate>Wed, 09 May 2018 11:11:48 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-regression_part3/</guid>
      <description>3.3 Other Considerations in the Regression Model 3.3.1 Qualitative Predictors There can be a case when predictor variables can be qualitative.
Predictors with Only Two Levels For the predictors with only two values, we can create an indicator or dummy variable with values 0 and 1 and use it in the regression model. The final prediction will not depend on the coding scheme. Only difference will be in the model coefficients and the way they are interpreted.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 3: Linear Regression (Part 2: Multiple Linear Regression)</title>
      <link>https://amitrajan012.github.io/post/linear-regression_part2/</link>
      <pubDate>Tue, 08 May 2018 06:12:03 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-regression_part2/</guid>
      <description>3.2 Multiple Linear Regression In general, suppose we have $p$ distinct predictors, the multiple linear regression takes the form:
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + &amp;hellip; + \beta_p X_p + \epsilon$$
where $\beta_j$ can be interpreted as the average effect on $Y$ of a one unit increase in $X_j$, holding all other predictors fixed.
3.2.1 Estimating the Regression Coefficients Given the estimates, $\widehat{\beta_0}, \widehat{\beta_1},&amp;hellip;, \widehat{\beta_p}$, predictions can be done as:</description>
    </item>
    
    <item>
      <title>ISLR Chapter 3: Linear Regression (Part 1: Simple Linear Regression)</title>
      <link>https://amitrajan012.github.io/post/linear-regression_part1/</link>
      <pubDate>Tue, 08 May 2018 02:18:09 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-regression_part1/</guid>
      <description>Linear Regression Linear Regression is a useful tool for predicting a quantitative response.
3.1 Simple Linear Regression Simple linear regression is a straightforward approach for predicting a quantitative response on the basis of a single predictor variable. Mathematically it can be written as:
$$Y \approx \beta_0 + \beta_1 X$$
$\beta_0$ and $\beta_1$ represent intercept and slope and are called as model coefficients or parameters. The estimated equation is given as:</description>
    </item>
    
    <item>
      <title>ISLR Chapter 2: Statistical Learning (Part 4: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/statistical-learning_part4/</link>
      <pubDate>Sun, 06 May 2018 07:24:34 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/statistical-learning_part4/</guid>
      <description>Applied Q8. This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US.
import pandas as pd  college = pd.read_csv(&amp;#34;data/College.csv&amp;#34;) college.set_index(&amp;#39;Unnamed: 0&amp;#39;, drop=True, inplace=True) college.index.names = [&amp;#39;Name&amp;#39;] college.describe()  .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }</description>
    </item>
    
    <item>
      <title>ISLR Chapter 2: Statistical Learning (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/statistical-learning_part3/</link>
      <pubDate>Sun, 06 May 2018 01:38:54 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/statistical-learning_part3/</guid>
      <description>2.4 Exercises Conceptual Q1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. 
(a) The sample size n is extremely large, and the number of predic- tors p is small.
Sol: Better
(b) The number of predictors p is extremely large, and the number of observations n is small.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 2: Statistical Learning (Part 2: Assessing Model Accuracy)</title>
      <link>https://amitrajan012.github.io/post/statistical-learning_part2/</link>
      <pubDate>Sat, 05 May 2018 07:11:10 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/statistical-learning_part2/</guid>
      <description>2.2 Assessing Model Accuracy No one statistical learning method dominates all other over all possible data sets. Hence it is an important task to decide that for any given data set which model fits best.
2.2.1 Measuring the Quality of Fit In the regression setting, the most commonly used measure for quality of fit is mean squared error (MSE), which is given as:
$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \widehat{f}(x_i))^{2}$$</description>
    </item>
    
    <item>
      <title>ISLR Chapter 2: Statistical Learning (Part 1: What Is Statistical Learning?)</title>
      <link>https://amitrajan012.github.io/post/statistical-learning_part1/</link>
      <pubDate>Fri, 04 May 2018 09:32:21 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/statistical-learning_part1/</guid>
      <description>Statistical Learning 2.1 What Is Statistical Learning? The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper. The plot of data is shown below. Our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 1: Introduction</title>
      <link>https://amitrajan012.github.io/post/introduction/</link>
      <pubDate>Wed, 02 May 2018 11:14:32 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/introduction/</guid>
      <description>Chapter 1: Introduction Wage Data import pandas as pd import matplotlib.pyplot as plt import seaborn as sns  wage = pd.read_csv(&amp;#34;data/Wage.csv&amp;#34;) wage.loc[df[&amp;#39;education&amp;#39;] == &amp;#39;1. &amp;lt; HS Grad&amp;#39;, &amp;#39;education&amp;#39;] = 1 wage.loc[df[&amp;#39;education&amp;#39;] == &amp;#39;2. HS Grad&amp;#39;, &amp;#39;education&amp;#39;] = 2 wage.loc[df[&amp;#39;education&amp;#39;] == &amp;#39;3. Some College&amp;#39;, &amp;#39;education&amp;#39;] = 3 wage.loc[df[&amp;#39;education&amp;#39;] == &amp;#39;4. College Grad&amp;#39;, &amp;#39;education&amp;#39;] = 4 wage.loc[df[&amp;#39;education&amp;#39;] == &amp;#39;5. Advanced Degree&amp;#39;, &amp;#39;education&amp;#39;] = 5 wage.head()  fig = plt.figure(figsize=(15,8))  ax = fig.</description>
    </item>
    
  </channel>
</rss>
