<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Matrix Space on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/matrix-space/</link>
    <description>Recent content in Matrix Space on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Apr 2022 14:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/matrix-space/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Orthonormal Vectors, Orthogonal Matrices and Gram-Schmidt Method</title>
      <link>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</link>
      <pubDate>Sat, 02 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</guid>
      <description>14.1 Orthonormal Vectors &amp;amp; Orthogonal Matrices A set of Orthonormal Vectors can be defined as:
$$\begin{align} q_i^Tq_j = \begin{cases} 0 ,&amp;amp; \text{if } i \neq j \\ 1 ,&amp;amp; \text{if } i=j \end{cases} \end{align}$$
When these set of $n$ orthonormal vectors are put into a matrix $Q$ such that $Q = \begin{bmatrix} q_1 &amp;amp; q_2 &amp;amp; &amp;hellip; &amp;amp; q_n \end{bmatrix}$, then we get $Q^TQ = I$. It should be noted that $Q$ doesn&amp;rsquo;t have to be a square matrix.</description>
    </item>
    
    <item>
      <title>Projection Matrices and Least Squares</title>
      <link>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</link>
      <pubDate>Wed, 30 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</guid>
      <description>13.1 Projection Matrices Let us look at the two extreme cases while taking the projection of vector $b$ onto the plane represented by matrix $A$. The projection matrix $P$ is given as: $P = A(A^TA)^{-1}A^T$.
  Case 1: When $b$ is $\perp$ to the column space of $A$, it&amp;rsquo;s projection $p=0$. This means that $b$ lies in the null space of $A^T$, i.e. $A^Tb=0$. Hence, $p = Pb = A(A^TA)^{-1}A^Tb = 0$.</description>
    </item>
    
    <item>
      <title>Projection of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</link>
      <pubDate>Sat, 26 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</guid>
      <description>12.1 Projections in one-dimensional Space Given two vectors $a,b$ in a plane, the projection of $b$ onto $a$ is shown below. The projection $p$ will be a multiple of $a$ and the error vector $e$ will be orthogonal to $a$. Error vector can be denoted as: $e = b - p$. As $e \perp a$, we can say that: $a^Te =0 \implies a^T(b - p) = 0 \implies a^T(b - xa) = 0 \implies xa^Ta = a^Tb \implies x = \frac{a^Tb}{a^Ta}$.</description>
    </item>
    
    <item>
      <title>Orthogonal Vectors and Orthogonal Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</link>
      <pubDate>Wed, 23 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</guid>
      <description>11.1 Orthogonal Vectors Orthogonal means perpendicular. For two vectors $x,y$, they are orthogonal if and only if $x^Ty=0$. As per Pythagoras Theorem, the test for orthogonality is: $\lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2$.
We can conclude the dot product condition from Pythoagoras Theorem as follows:
$$\begin{align} \lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2 \\ x^Tx + y^Ty = (x+y)^T(x+y) \\ x^Tx + y^Ty = x^Tx + y^Ty + x^Ty + y^Tx \\ x^Ty + y^Tx = 0 \\ 2x^Ty = 0 \\ x^Ty = 0 \end{align}$$</description>
    </item>
    
    <item>
      <title>Graphs, Networks and Incidence Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</link>
      <pubDate>Mon, 21 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</guid>
      <description>10.1 Graphs, Networks and Incidence Matrices: Graphs consist of nodes and edges. For example, in the attached figure, the graph has $n=4$ nodes and $m=5$ edges. The graph can be interpreted as a circuit where nodes represent the points from which current flows and edges with the arrow represent the direction of its flow.
The above graph can be represented using a matrix, called as Incidence Matrix. The edges of the graph are represented using rows where entry at each column (one columnn for each of the node) index represents the start and end of the edge.</description>
    </item>
    
    <item>
      <title>Matrix Spaces</title>
      <link>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</link>
      <pubDate>Thu, 17 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</guid>
      <description>9.1 Matrix Spaces The idea of vector spaces can be extended to matrices as well as far as they follow the following properties
 If $A \in S$ then $cA \in S$ If $A \in S; B \in S$ then $A+B \in S$  It should be noted that the matrix multiplication doesn&amp;rsquo;t need to belong to the same matrix space.
For a matrix $M$, some of the examples of matrix spaces are: Upper Triangular Mtrices, Symmetric Matrices, Diagonal Matrices etc.</description>
    </item>
    
  </channel>
</rss>
