<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Algebra on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/linear-algebra/</link>
    <description>Recent content in Linear Algebra on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 02 Apr 2022 14:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Orthonormal Vectors, Orthogonal Matrices and Gram-Schmidt Method</title>
      <link>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</link>
      <pubDate>Sat, 02 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</guid>
      <description>14.1 Orthonormal Vectors &amp;amp; Orthogonal Matrices A set of Orthonormal Vectors can be defined as:
$$\begin{align} q_i^Tq_j = \begin{cases} 0 ,&amp;amp; \text{if } i \neq j \\ 1 ,&amp;amp; \text{if } i=j \end{cases} \end{align}$$
When these set of $n$ orthonormal vectors are put into a matrix $Q$ such that $Q = \begin{bmatrix} q_1 &amp;amp; q_2 &amp;amp; &amp;hellip; &amp;amp; q_n \end{bmatrix}$, then we get $Q^TQ = I$. It should be noted that $Q$ doesn&amp;rsquo;t have to be a square matrix.</description>
    </item>
    
    <item>
      <title>Projection Matrices and Least Squares</title>
      <link>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</link>
      <pubDate>Wed, 30 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</guid>
      <description>13.1 Projection Matrices Let us look at the two extreme cases while taking the projection of vector $b$ onto the plane represented by matrix $A$. The projection matrix $P$ is given as: $P = A(A^TA)^{-1}A^T$.
  Case 1: When $b$ is $\perp$ to the column space of $A$, it&amp;rsquo;s projection $p=0$. This means that $b$ lies in the null space of $A^T$, i.e. $A^Tb=0$. Hence, $p = Pb = A(A^TA)^{-1}A^Tb = 0$.</description>
    </item>
    
    <item>
      <title>Projection of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</link>
      <pubDate>Sat, 26 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</guid>
      <description>12.1 Projections in one-dimensional Space Given two vectors $a,b$ in a plane, the projection of $b$ onto $a$ is shown below. The projection $p$ will be a multiple of $a$ and the error vector $e$ will be orthogonal to $a$. Error vector can be denoted as: $e = b - p$. As $e \perp a$, we can say that: $a^Te =0 \implies a^T(b - p) = 0 \implies a^T(b - xa) = 0 \implies xa^Ta = a^Tb \implies x = \frac{a^Tb}{a^Ta}$.</description>
    </item>
    
    <item>
      <title>Orthogonal Vectors and Orthogonal Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</link>
      <pubDate>Wed, 23 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</guid>
      <description>11.1 Orthogonal Vectors Orthogonal means perpendicular. For two vectors $x,y$, they are orthogonal if and only if $x^Ty=0$. As per Pythagoras Theorem, the test for orthogonality is: $\lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2$.
We can conclude the dot product condition from Pythoagoras Theorem as follows:
$$\begin{align} \lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2 \\ x^Tx + y^Ty = (x+y)^T(x+y) \\ x^Tx + y^Ty = x^Tx + y^Ty + x^Ty + y^Tx \\ x^Ty + y^Tx = 0 \\ 2x^Ty = 0 \\ x^Ty = 0 \end{align}$$</description>
    </item>
    
    <item>
      <title>Graphs, Networks and Incidence Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</link>
      <pubDate>Mon, 21 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</guid>
      <description>10.1 Graphs, Networks and Incidence Matrices: Graphs consist of nodes and edges. For example, in the attached figure, the graph has $n=4$ nodes and $m=5$ edges. The graph can be interpreted as a circuit where nodes represent the points from which current flows and edges with the arrow represent the direction of its flow.
The above graph can be represented using a matrix, called as Incidence Matrix. The edges of the graph are represented using rows where entry at each column (one columnn for each of the node) index represents the start and end of the edge.</description>
    </item>
    
    <item>
      <title>Matrix Spaces</title>
      <link>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</link>
      <pubDate>Thu, 17 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</guid>
      <description>9.1 Matrix Spaces The idea of vector spaces can be extended to matrices as well as far as they follow the following properties
 If $A \in S$ then $cA \in S$ If $A \in S; B \in S$ then $A+B \in S$  It should be noted that the matrix multiplication doesn&amp;rsquo;t need to belong to the same matrix space.
For a matrix $M$, some of the examples of matrix spaces are: Upper Triangular Mtrices, Symmetric Matrices, Diagonal Matrices etc.</description>
    </item>
    
    <item>
      <title>Four Fundamental Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_8-_the_four_fundamental_subspaces/</link>
      <pubDate>Tue, 15 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_8-_the_four_fundamental_subspaces/</guid>
      <description>8.1 Four Fundamental Subspaces The four fundamental subspaces for a $m \times n$ matrix $A$ are as follows:
 Column Space $C(A)$ in $\mathbb{R}^m$ Null Space $N(A)$ in $\mathbb{R}^n$: Solution to $Ax=0$ Row Space $C(A^T)$ in $\mathbb{R}^n$: All combinations of the rows of $A$ or we can say that all combinations of the columns of $A^T$ Left Null Space of $A^T$ $N(A^T)$ in $\mathbb{R}^m$: Solution to $A^Ty=0$ and is also called as Left Null Spcae of $A$  The pictorial representation of these spaces with their dimension and basis is as follows:</description>
    </item>
    
    <item>
      <title>Matrix Independence, Span, Basis &amp; Dimension</title>
      <link>https://amitrajan012.github.io/post/chapter_7_independence_basis_and_dimension/</link>
      <pubDate>Sun, 30 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_7_independence_basis_and_dimension/</guid>
      <description>7.1 Independence Vectors $x_1,x_2, &amp;hellip;, x_n$ are linearly independent if no combinations of these vectors gives zero vector, except the zero combination. i.e.
$$\begin{align} c_1x_1 + c_2x_2 + &amp;hellip; + c_nx_n \neq 0; except \ \forall c_i=0 \end{align}$$
As we know that, for a $m \times n$ matrix $A$, if $m &amp;lt; n$, then we will have at least one non-zero solution to $Ax=0$. It implies that we will always find some non-zero combinations of $c_i$ that will make satisfy the above condition.</description>
    </item>
    
    <item>
      <title>Algorithm for solving $Ax=b$</title>
      <link>https://amitrajan012.github.io/post/chapter_6_solving_for_axb/</link>
      <pubDate>Tue, 25 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_6_solving_for_axb/</guid>
      <description>6.1 Algorithm for solving $Ax=b$ The idea behind this exercise is to come up with the solution for $Ax=b$. Let us start by taking a matrix $A$ and vector $b$.
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 \\ 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \end{bmatrix}; b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} \end{align}$$
The augumented matrix $\begin{bmatrix} A &amp;amp; b \end{bmatrix}$ can be formed and elimination steps can be performed on it as follows.</description>
    </item>
    
    <item>
      <title>Algorithm for solving $Ax=0$</title>
      <link>https://amitrajan012.github.io/post/chapter_5_computing_nullspace/</link>
      <pubDate>Thu, 20 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_5_computing_nullspace/</guid>
      <description>5.1 Algorithm for solving $Ax=0$ The idea behind this exercise is to come up with an algorithm to find the nullspace of a matrix $A$. Let us start by taking a matrix $A$.
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 \\ 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \end{bmatrix} \end{align}$$
One of the first thing to notice in $A$ is that $row_3$ is a linear combination of $row_1$ and $row_2$ ($row_3 = row_1 + row_2$).</description>
    </item>
    
    <item>
      <title>Vector Space and Subspace</title>
      <link>https://amitrajan012.github.io/post/chapter_4_vector_spaces/</link>
      <pubDate>Mon, 17 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_4_vector_spaces/</guid>
      <description>4.1 Vector Space and Subspace Vector Space is a set of vectors whose linear combinations belong to the same set. This means that whenever we pick $2$ vectors from a vector space and find thier linear combination, the resultant vector will be in the vector space. A vector space in two and three dimensions is $R^2$ and $R^3$.
We can make some common observations about vector spaces. Let $u,v$ be two vectors, then their linear combination can be described as $w=au+bv$ where $a,b$ are scalars.</description>
    </item>
    
    <item>
      <title>Inverse of a Matrix &amp; Factorization into $A=LU$</title>
      <link>https://amitrajan012.github.io/post/chapter_3_inverse_factorization_and_symmetric_matrices/</link>
      <pubDate>Wed, 12 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_3_inverse_factorization_and_symmetric_matrices/</guid>
      <description>3.1 Inverse of a Matrix &amp;amp; Factorization into $A=LU$ Given a square matrix $A$, $A^{-1}$ is it&amp;rsquo;s inverse if $AA^{-1} = A^{-1}A = I$, where $I$ is the Identity Matrix and we say that $A$ is invertible or nonsingular.
For a singular matrix $A$, the inverse does not exist and it&amp;rsquo;s determinant is $0$. Also, we can find a vector $x$ such that $Ax=0$. This means that one or more than one columns of $A$ is linear combination of other columns.</description>
    </item>
    
    <item>
      <title>Elimination &amp; Permutation with Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_2_elimination_and_permutation/</link>
      <pubDate>Sat, 08 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_2_elimination_and_permutation/</guid>
      <description>2.1 Elimination &amp;amp; Permutation with Matrices Elimination is the method which is used to solve a system of linear equations. Let $Ax=b$ is a system of linear equation where
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 1\\ 3 &amp;amp; 8 &amp;amp; 1\\ 0 &amp;amp; 4 &amp;amp; 1 \end{bmatrix}, b = \begin{bmatrix} 2 \\ 12 \\ 2 \end{bmatrix} \end{align}$$
The idea of elimination is to transform $A$ into a matrix where all the entries in the cell below the diagonal is 0.</description>
    </item>
    
    <item>
      <title>Geometry of Linear Equations &amp; Matrix Multiplications</title>
      <link>https://amitrajan012.github.io/post/chapter_1_geometry_of_linear_equations__matrix_multiplications/</link>
      <pubDate>Mon, 03 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_1_geometry_of_linear_equations__matrix_multiplications/</guid>
      <description>1.1 The Geometry of Linear Equations The fundamental goal of linear algebra is to solve a system of linear equations. Let us look at an example of a set of $2$ linear equations in $2$ unknowns:
$$\begin{align} 2x-y = 0 \\ -x+2y = 3 \end{align}$$
The above system of linear equation when written in matrix multiplication form can be represented as $Ax = b$ where $A$ is a $2 \times 2$ matrix, $x$ and $b$ are column vectors.</description>
    </item>
    
  </channel>
</rss>
