<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Linear Algebra on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/linear-algebra/</link>
    <description>Recent content in Linear Algebra on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 May 2022 14:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Left, Right  and Pseudo Inverses</title>
      <link>https://amitrajan012.github.io/post/chapter-28-left-and-right-inverses-and-pseudoinverse/</link>
      <pubDate>Sun, 15 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-28-left-and-right-inverses-and-pseudoinverse/</guid>
      <description>28.1 Left and Right Inverses For any matrix $A$, there are four fundamental subspaces: Row Space, Null Space, Column Space, Null Space of $A^T$. Row Space and Null Space combined span entire $\mathbb{R}^n$ for a $m \times n$ matrix $A$. Column Space and Null Space of $A^T$ combined span entire $\mathbb{R}^m$ for a $m \times n$ matrix $A$. For the inverse of the matrix to exist, it&amp;rsquo;s rank should be $r=m=n$ whtere the dimension of the matrix is $m \times n$.</description>
    </item>
    
    <item>
      <title>Linear Transformations, Change of Basis and Image Compression</title>
      <link>https://amitrajan012.github.io/post/chapter-27-linear-transformations-and-their-matrices-change-of-basis-and-image-compression/</link>
      <pubDate>Thu, 12 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-27-linear-transformations-and-their-matrices-change-of-basis-and-image-compression/</guid>
      <description>27.1 Linear Transformations When we are dealing with coordinates, every linear transformation leads us to a matrix. For any vector $v$ and $w$, linear transformation $T$ follows following properties:
 $T(v+w) = T(v) + T(w)$ $T(cv) = cT(v)$ $T(0) = T(0)$, derived from tha above two properties  Below are some examples and non-examples of linear transformation:
Example 1: Projection In a two-dimensional space $\mathbb{R}^2$, a projection of a vector $v$ on a line is linear transformation and can be denoted as $T:\mathbb{R}^2 \to \mathbb{R}^2$.</description>
    </item>
    
    <item>
      <title>Singular Value Decomposition</title>
      <link>https://amitrajan012.github.io/post/chapter-26-singular-value-decomposition/</link>
      <pubDate>Mon, 09 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-26-singular-value-decomposition/</guid>
      <description>26.1 Singular Value Decomposition If any matrix $A$ can be decomposed as $A = U\Sigma V^T$, where $\Sigma$ is a diagonal matrix and $U,V$ are orthogonal matrices, this is called as Singular Value Decomposition (SVD). One of the examples of SVD is for a symmetric positive definite matrix $A$, we know that $A = Q\Lambda Q^T$, where $\Lambda$ is diagonal and $Q$ is orthogonal.
For a matrix $m \times n$ matrix $A$, let the row-space be entire $\mathbb{R}^n$ and the column-space be entire $\mathbb{R}^m$.</description>
    </item>
    
    <item>
      <title>Similar Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter-25-similar-matrices-and-jordan-form/</link>
      <pubDate>Fri, 06 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-25-similar-matrices-and-jordan-form/</guid>
      <description>25.1 Similar Matrices One of the major generator of a positive definite matrix is when we square one. In a nutsheel, the matrix $A^TA$ is always positive definite. To prove this, let $A$ be a $m \times n$ rectangular matrix. Then $A^TA$ is a square symmetric matrix. If we evaluate the expression $x^T(A^TA)x$, we get $x^T(A^TA)x = (x^TA^T)(Ax) = (Ax)^T(Ax) = |Ax|^2 &amp;gt; 0$ if $Ax$ is non-zero. Another important thing to note is if matrix $A, B$ are positive definite, $A+B$ is positive definite.</description>
    </item>
    
    <item>
      <title>Positive Definite Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter-24-positive-definite-matrices-and-minima/</link>
      <pubDate>Mon, 02 May 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-24-positive-definite-matrices-and-minima/</guid>
      <description>24.1 Positive Definite Matrices These are the complete tests for a $2 \times 2$ matrix $A = \begin{bmatrix} a &amp;amp; b \\ b &amp;amp; c \end{bmatrix}$ for being Positive Definite:
 Both the eigenvalues should be positive: $\lambda_1 &amp;gt; 0;\lambda_2 &amp;gt; 0$ All the sub-determinants should be positive: $a &amp;gt; 0; ac - b^2 &amp;gt; 0$ Pivots should be positive: $a&amp;gt;0;\frac{ac-b^2}{a} &amp;gt; 0$ $x^TAx &amp;gt; 0;\forall x$  The matrix for which any of these conditions holds with equality instead are called as positive semi-definite matrices.</description>
    </item>
    
    <item>
      <title>Complex Matrices and Fourier Transform</title>
      <link>https://amitrajan012.github.io/post/chapter-23-complex-matrices-and-fast-fourier-transform/</link>
      <pubDate>Fri, 29 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter-23-complex-matrices-and-fast-fourier-transform/</guid>
      <description>23.1 Complex Vectors/Matrices Let $z = \begin{bmatrix} z_1 \\ z_2 \\ &amp;hellip; \\ z_n \end{bmatrix}$ be a complex vector in $C^n$. The expresson $z^Tz$ doesn&amp;rsquo;t represent the length of the vector $z$. Instead it&amp;rsquo;s length is represented by $\overline{z}^Tz$. $\overline{z}^T$ is also called as $z^H$ and is called as Hermitian of a matrix. Hence, the length of a complex vector is $z^Hz = |z_1|^2 + |z_2|^2 + &amp;hellip; + |z_n|^2$. Similarly, for a complex matrix $A$ to be symmetric, $A^H = A$ with diagonal elements being real.</description>
    </item>
    
    <item>
      <title>Symmetric Matrices and Positive Definiteness</title>
      <link>https://amitrajan012.github.io/post/chapter_22_symmetric_matrices_and_positive_definiteness/</link>
      <pubDate>Tue, 26 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_22_symmetric_matrices_and_positive_definiteness/</guid>
      <description>22.1 Symmetric Matrices For a Symmetric Matrix $A$, $A = A^T$. Eigenvalues of real Symmetric Matrices are real and eigenvectors are perpendicular. Any matrix $A$ can be written as $A = S\Lambda S^{-1}$. For a symmetric matrix $A$, this relationship reduces to $A = Q \Lambda Q^{-1}$ as $S$, which is the eigenvector matrix has orthonormal eigenvectors. For an orthonormal matrix, $Q^{-1} = Q^T$ and hence $A = Q\Lambda Q^T$. This is called as the Spectral Theorem in mathematics.</description>
    </item>
    
    <item>
      <title>Markov Matrices and Fourier Series</title>
      <link>https://amitrajan012.github.io/post/chapter_21_markov_matrices_and_fourier_series/</link>
      <pubDate>Fri, 22 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_21_markov_matrices_and_fourier_series/</guid>
      <description>21.1 Markov Matrices Markov Matrices have the following properties:
 All entries $\geq 0$ Sum of the entries in a column equal $1$  A markov matrix will always have an eigenvalue of $1$. Apart from this, all other eigenvalues will be $\leq 1$.
Let us consider the difference equation $u_k = A^ku_0$ where we represent $u_0$ as the combinations of eigenvectors, i.e. $u_0 = c_1x_1 + c_2x_2 + &amp;hellip; + c_nx_n = Sc$.</description>
    </item>
    
    <item>
      <title>Differential Equations and Matrix Exponentials</title>
      <link>https://amitrajan012.github.io/post/chapter_20_differential_equations_and_expat/</link>
      <pubDate>Wed, 20 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_20_differential_equations_and_expat/</guid>
      <description>20.1 Differential Equations $\frac{du}{dt} = Au$ Example: Let the system of differential equation to be solved is: $\frac{du_1}{dt} = -u_1 + 2u_2; \frac{du_2}{dt} = u_1 - 2u_2$ with initial condition of $u(0) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$. The matrix $A$ representing the coefficients of the equation is $A = \begin{bmatrix} -1 &amp;amp; 2 \\ 1 &amp;amp; -2 \end{bmatrix}$. The eigenvalues of the matrix $A$ satisfies the equation $\lambda_1 + \lambda_2 = -3; \lambda_1 \times \lambda_2 = 0$, i.</description>
    </item>
    
    <item>
      <title>Diagonalization and Powers of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_19_diagonalization_and_powers_of_a_matrix/</link>
      <pubDate>Sun, 17 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_19_diagonalization_and_powers_of_a_matrix/</guid>
      <description>19.1 Diagonalization of a Matrix Suppose for a given matrix $A$, we have $n$ linearlly independent eigenvectros and we put them in a matrix $S$. Then $AS = A \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix}$, where each $x_i$ is the eigenvector. But for each of the eigenvectors, $Ax_i = \lambda_ix_i$. Hence, $AS = A \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix} = \begin{bmatrix} \lambda_1x_1 &amp;amp; \lambda_2x_2 &amp;amp; &amp;hellip; &amp;amp; \lambda_nx_n \end{bmatrix} = \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix}diag(\lambda_i) = S\Lambda$.</description>
    </item>
    
    <item>
      <title>Eigenvalues and Eigenvectors</title>
      <link>https://amitrajan012.github.io/post/chapter_18_eigenvalues_and_eigenvectors/</link>
      <pubDate>Thu, 14 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_18_eigenvalues_and_eigenvectors/</guid>
      <description>18.1 Eigenvalues and Eigenvectors The usual function of a matrix is to act on a vector like a function, i.e. in goes a vector $x$, out comes a vector $Ax$. For a specific matrix $A$, if $Ax$ is parallel to $x$ i.e. $Ax = \lambda x$, the vectors $x$ are called as eigenvectors. The constant $\lambda$ is called as eigenvalue.
The vector $x$ and constant $\lambda$ satisfying the equation $Ax = \lambda x$ are eigenvectors and eigenvalues.</description>
    </item>
    
    <item>
      <title>Formula for $A^{-1}$ and Cramer&#39;s Rule</title>
      <link>https://amitrajan012.github.io/post/chapter_17_cramers_rule_inverse_matrix_and_volume/</link>
      <pubDate>Sun, 10 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_17_cramers_rule_inverse_matrix_and_volume/</guid>
      <description>17.1 Formula for $A^{-1}$ $A^{-1}$ can be given as $A^{-1} = \frac{1}{|A|}C^T$, where $C^T$ is the matrix of cofactors transposed. To verify this formula, we have to check that $AA^{-1}=I$, or $AC^T=|A|I$. If we expand the left hand side, we get
$$\begin{align} \begin{bmatrix} a_{11} &amp;amp; &amp;hellip; &amp;amp; a_{1n}\\ : &amp;amp; : &amp;amp; :\\ a_{n1} &amp;amp; &amp;hellip; &amp;amp; a_{nn} \end{bmatrix}\begin{bmatrix} C_{11} &amp;amp; &amp;hellip; &amp;amp; C_{n1}\\ : &amp;amp; : &amp;amp; :\\ C_{1n} &amp;amp; &amp;hellip; &amp;amp; C_{nn} \end{bmatrix}=\begin{bmatrix} |A| &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; |A| &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; |A| \end{bmatrix}=|A|I \end{align}$$</description>
    </item>
    
    <item>
      <title>Determinant and Cofactors</title>
      <link>https://amitrajan012.github.io/post/chapter_16_determinant_formulas_and_cofactors/</link>
      <pubDate>Thu, 07 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_16_determinant_formulas_and_cofactors/</guid>
      <description>16.1 Formula for $|A|$ For a $2 \times 2$ matrix $A$, the formula for $|A|$ can be derived as follows:
$$\begin{align} |A| = \begin{vmatrix} a &amp;amp; b \\ c &amp;amp; d \end{vmatrix}= \begin{vmatrix} a &amp;amp; 0 \\ c &amp;amp; d \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ c &amp;amp; d \end{vmatrix} \end{align}$$
$$\begin{align} =\begin{vmatrix} a &amp;amp; 0 \\ c &amp;amp; 0 \end{vmatrix}+\begin{vmatrix} a &amp;amp; 0 \\ 0 &amp;amp; d \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ c &amp;amp; 0 \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ 0 &amp;amp; d \end{vmatrix} \end{align}$$</description>
    </item>
    
    <item>
      <title>Determinant</title>
      <link>https://amitrajan012.github.io/post/chapter_15_properties_of_determinants/</link>
      <pubDate>Tue, 05 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_15_properties_of_determinants/</guid>
      <description>15.1 Determinant Every square matrix has a number assosiated with it can we call this number as determinant, often denoted as $det(A) = |A|$ for matrix $A$.
Prperties of determinant is as follows:
  $|I|=1$
  Row exchange reverses the sign of the determinant:
  Permutation Matrices are derived by row exchange of Identity Matrix. Hence, $|P|= \pm1$.
 (a) For any square matrix $A$, $\begin{vmatrix} ta &amp;amp; tb \\ c &amp;amp; d \end{vmatrix}=t\begin{vmatrix} a &amp;amp; b \\ c &amp;amp; d \end{vmatrix}$</description>
    </item>
    
    <item>
      <title>Orthonormal Vectors, Orthogonal Matrices and Gram-Schmidt Method</title>
      <link>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</link>
      <pubDate>Sat, 02 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_14_orthogonal_matrices_orthonormal_vectors_and_gram-schmidt/</guid>
      <description>14.1 Orthonormal Vectors &amp;amp; Orthogonal Matrices A set of Orthonormal Vectors can be defined as:
$$\begin{align} q_i^Tq_j = \begin{cases} 0 ,&amp;amp; \text{if } i \neq j \\ 1 ,&amp;amp; \text{if } i=j \end{cases} \end{align}$$
When these set of $n$ orthonormal vectors are put into a matrix $Q$ such that $Q = \begin{bmatrix} q_1 &amp;amp; q_2 &amp;amp; &amp;hellip; &amp;amp; q_n \end{bmatrix}$, then we get $Q^TQ = I$. It should be noted that $Q$ doesn&amp;rsquo;t have to be a square matrix.</description>
    </item>
    
    <item>
      <title>Projection Matrices and Least Squares</title>
      <link>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</link>
      <pubDate>Wed, 30 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_13_projection_matrices_and_least_square/</guid>
      <description>13.1 Projection Matrices Let us look at the two extreme cases while taking the projection of vector $b$ onto the plane represented by matrix $A$. The projection matrix $P$ is given as: $P = A(A^TA)^{-1}A^T$.
  Case 1: When $b$ is $\perp$ to the column space of $A$, it&amp;rsquo;s projection $p=0$. This means that $b$ lies in the null space of $A^T$, i.e. $A^Tb=0$. Hence, $p = Pb = A(A^TA)^{-1}A^Tb = 0$.</description>
    </item>
    
    <item>
      <title>Projection of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</link>
      <pubDate>Sat, 26 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_12_projection_onto_subspaces/</guid>
      <description>12.1 Projections in one-dimensional Space Given two vectors $a,b$ in a plane, the projection of $b$ onto $a$ is shown below. The projection $p$ will be a multiple of $a$ and the error vector $e$ will be orthogonal to $a$. Error vector can be denoted as: $e = b - p$. As $e \perp a$, we can say that: $a^Te =0 \implies a^T(b - p) = 0 \implies a^T(b - xa) = 0 \implies xa^Ta = a^Tb \implies x = \frac{a^Tb}{a^Ta}$.</description>
    </item>
    
    <item>
      <title>Orthogonal Vectors and Orthogonal Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</link>
      <pubDate>Wed, 23 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_11_orthogonal_vectors_and_subspaces/</guid>
      <description>11.1 Orthogonal Vectors Orthogonal means perpendicular. For two vectors $x,y$, they are orthogonal if and only if $x^Ty=0$. As per Pythagoras Theorem, the test for orthogonality is: $\lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2$.
We can conclude the dot product condition from Pythoagoras Theorem as follows:
$$\begin{align} \lVert x \rVert^2 + \lVert y \rVert^2 = \lVert x+y \rVert^2 \\ x^Tx + y^Ty = (x+y)^T(x+y) \\ x^Tx + y^Ty = x^Tx + y^Ty + x^Ty + y^Tx \\ x^Ty + y^Tx = 0 \\ 2x^Ty = 0 \\ x^Ty = 0 \end{align}$$</description>
    </item>
    
    <item>
      <title>Graphs, Networks and Incidence Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</link>
      <pubDate>Mon, 21 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_10_graphs_networks_and_incidence_matrices/</guid>
      <description>10.1 Graphs, Networks and Incidence Matrices: Graphs consist of nodes and edges. For example, in the attached figure, the graph has $n=4$ nodes and $m=5$ edges. The graph can be interpreted as a circuit where nodes represent the points from which current flows and edges with the arrow represent the direction of its flow.
The above graph can be represented using a matrix, called as Incidence Matrix. The edges of the graph are represented using rows where entry at each column (one columnn for each of the node) index represents the start and end of the edge.</description>
    </item>
    
    <item>
      <title>Matrix Spaces</title>
      <link>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</link>
      <pubDate>Thu, 17 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_9_matrix_spaces/</guid>
      <description>9.1 Matrix Spaces The idea of vector spaces can be extended to matrices as well as far as they follow the following properties
 If $A \in S$ then $cA \in S$ If $A \in S; B \in S$ then $A+B \in S$  It should be noted that the matrix multiplication doesn&amp;rsquo;t need to belong to the same matrix space.
For a matrix $M$, some of the examples of matrix spaces are: Upper Triangular Mtrices, Symmetric Matrices, Diagonal Matrices etc.</description>
    </item>
    
    <item>
      <title>Four Fundamental Subspaces</title>
      <link>https://amitrajan012.github.io/post/chapter_8-_the_four_fundamental_subspaces/</link>
      <pubDate>Tue, 15 Mar 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_8-_the_four_fundamental_subspaces/</guid>
      <description>8.1 Four Fundamental Subspaces The four fundamental subspaces for a $m \times n$ matrix $A$ are as follows:
 Column Space $C(A)$ in $\mathbb{R}^m$ Null Space $N(A)$ in $\mathbb{R}^n$: Solution to $Ax=0$ Row Space $C(A^T)$ in $\mathbb{R}^n$: All combinations of the rows of $A$ or we can say that all combinations of the columns of $A^T$ Left Null Space of $A^T$ $N(A^T)$ in $\mathbb{R}^m$: Solution to $A^Ty=0$ and is also called as Left Null Spcae of $A$  The pictorial representation of these spaces with their dimension and basis is as follows:</description>
    </item>
    
    <item>
      <title>Matrix Independence, Span, Basis &amp; Dimension</title>
      <link>https://amitrajan012.github.io/post/chapter_7_independence_basis_and_dimension/</link>
      <pubDate>Sun, 30 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_7_independence_basis_and_dimension/</guid>
      <description>7.1 Independence Vectors $x_1,x_2, &amp;hellip;, x_n$ are linearly independent if no combinations of these vectors gives zero vector, except the zero combination. i.e.
$$\begin{align} c_1x_1 + c_2x_2 + &amp;hellip; + c_nx_n \neq 0; except \ \forall c_i=0 \end{align}$$
As we know that, for a $m \times n$ matrix $A$, if $m &amp;lt; n$, then we will have at least one non-zero solution to $Ax=0$. It implies that we will always find some non-zero combinations of $c_i$ that will make satisfy the above condition.</description>
    </item>
    
    <item>
      <title>Algorithm for solving $Ax=b$</title>
      <link>https://amitrajan012.github.io/post/chapter_6_solving_for_axb/</link>
      <pubDate>Tue, 25 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_6_solving_for_axb/</guid>
      <description>6.1 Algorithm for solving $Ax=b$ The idea behind this exercise is to come up with the solution for $Ax=b$. Let us start by taking a matrix $A$ and vector $b$.
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 \\ 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \end{bmatrix}; b = \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} \end{align}$$
The augumented matrix $\begin{bmatrix} A &amp;amp; b \end{bmatrix}$ can be formed and elimination steps can be performed on it as follows.</description>
    </item>
    
    <item>
      <title>Algorithm for solving $Ax=0$</title>
      <link>https://amitrajan012.github.io/post/chapter_5_computing_nullspace/</link>
      <pubDate>Thu, 20 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_5_computing_nullspace/</guid>
      <description>5.1 Algorithm for solving $Ax=0$ The idea behind this exercise is to come up with an algorithm to find the nullspace of a matrix $A$. Let us start by taking a matrix $A$.
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 2 &amp;amp; 2 \\ 2 &amp;amp; 4 &amp;amp; 6 &amp;amp; 8 \\ 3 &amp;amp; 6 &amp;amp; 8 &amp;amp; 10 \end{bmatrix} \end{align}$$
One of the first thing to notice in $A$ is that $row_3$ is a linear combination of $row_1$ and $row_2$ ($row_3 = row_1 + row_2$).</description>
    </item>
    
    <item>
      <title>Vector Space and Subspace</title>
      <link>https://amitrajan012.github.io/post/chapter_4_vector_spaces/</link>
      <pubDate>Mon, 17 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_4_vector_spaces/</guid>
      <description>4.1 Vector Space and Subspace Vector Space is a set of vectors whose linear combinations belong to the same set. This means that whenever we pick $2$ vectors from a vector space and find thier linear combination, the resultant vector will be in the vector space. A vector space in two and three dimensions is $R^2$ and $R^3$.
We can make some common observations about vector spaces. Let $u,v$ be two vectors, then their linear combination can be described as $w=au+bv$ where $a,b$ are scalars.</description>
    </item>
    
    <item>
      <title>Inverse of a Matrix &amp; Factorization into $A=LU$</title>
      <link>https://amitrajan012.github.io/post/chapter_3_inverse_factorization_and_symmetric_matrices/</link>
      <pubDate>Wed, 12 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_3_inverse_factorization_and_symmetric_matrices/</guid>
      <description>3.1 Inverse of a Matrix &amp;amp; Factorization into $A=LU$ Given a square matrix $A$, $A^{-1}$ is it&amp;rsquo;s inverse if $AA^{-1} = A^{-1}A = I$, where $I$ is the Identity Matrix and we say that $A$ is invertible or nonsingular.
For a singular matrix $A$, the inverse does not exist and it&amp;rsquo;s determinant is $0$. Also, we can find a vector $x$ such that $Ax=0$. This means that one or more than one columns of $A$ is linear combination of other columns.</description>
    </item>
    
    <item>
      <title>Elimination &amp; Permutation with Matrices</title>
      <link>https://amitrajan012.github.io/post/chapter_2_elimination_and_permutation/</link>
      <pubDate>Sat, 08 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_2_elimination_and_permutation/</guid>
      <description>2.1 Elimination &amp;amp; Permutation with Matrices Elimination is the method which is used to solve a system of linear equations. Let $Ax=b$ is a system of linear equation where
$$\begin{align} A = \begin{bmatrix} 1 &amp;amp; 2 &amp;amp; 1\\ 3 &amp;amp; 8 &amp;amp; 1\\ 0 &amp;amp; 4 &amp;amp; 1 \end{bmatrix}, b = \begin{bmatrix} 2 \\ 12 \\ 2 \end{bmatrix} \end{align}$$
The idea of elimination is to transform $A$ into a matrix where all the entries in the cell below the diagonal is 0.</description>
    </item>
    
    <item>
      <title>Geometry of Linear Equations &amp; Matrix Multiplications</title>
      <link>https://amitrajan012.github.io/post/chapter_1_geometry_of_linear_equations__matrix_multiplications/</link>
      <pubDate>Mon, 03 Jan 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_1_geometry_of_linear_equations__matrix_multiplications/</guid>
      <description>1.1 The Geometry of Linear Equations The fundamental goal of linear algebra is to solve a system of linear equations. Let us look at an example of a set of $2$ linear equations in $2$ unknowns:
$$\begin{align} 2x-y = 0 \\ -x+2y = 3 \end{align}$$
The above system of linear equation when written in matrix multiplication form can be represented as $Ax = b$ where $A$ is a $2 \times 2$ matrix, $x$ and $b$ are column vectors.</description>
    </item>
    
  </channel>
</rss>
