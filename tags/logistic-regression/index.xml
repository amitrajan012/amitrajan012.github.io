<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Logistic Regression on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/logistic-regression/</link>
    <description>Recent content in Logistic Regression on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Aug 2022 23:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/logistic-regression/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sparse Kernel Methods - Maximum Margin Classifiers: Relation to Logistic Regression, Multiclass SVMs, SVMs for Regression</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-7-sparse-kernel-machines_4/</link>
      <pubDate>Sun, 28 Aug 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-7-sparse-kernel-machines_4/</guid>
      <description>7.1.2 Relation to Logistic Regression For nonseparable case of SVM, for the data points that are on the correct side of margin, we have $\xi_n=0$ and hence $t_ny_n \geq 1$. For the remaining points, we have $\xi_n = 1 - t_ny_n$. The below objective function can then be written as
$$\begin{align} \frac{1}{2}||W||^2 + C\sum_{n=1}^{N}\xi_n = \lambda||W||^2 + \sum_{n=1}^{N}E_{SV}(t_ny_n) \end{align}$$
where $\lambda = (2C)^{-1}$ and $E_{SV}(.)$ is the hinge error function defined as</description>
    </item>
    
    <item>
      <title>Linear Models for Clasification - Probabilistic Discriminative Models</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-4-linear-models-for-classification_8/</link>
      <pubDate>Tue, 05 Jul 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-4-linear-models-for-classification_8/</guid>
      <description>4.3 Probabilistic Discriminative Models For two-class classification problem, the posterior probability of classes are the logistic sigmoid transformation of a linear function of input $X$. For multi-class classification problem, they are given by the softmax transformation of a linear function of input $X$. In maximum likelihood solution, we chose the class-conditional densities and then maximized the log likelihood to obtain posterior densities. However, an alternative approach is to use the functional form of the generalized linear model explicitly instead and to determine its parameters directly by using maximum likelihood.</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://amitrajan012.github.io/post/logistic-regression/</link>
      <pubDate>Wed, 05 Dec 2018 07:26:51 +0530</pubDate>
      
      <guid>https://amitrajan012.github.io/post/logistic-regression/</guid>
      <description>In a classification setting, logistic regression models the probability of a response $Y$ belonging to a particaular category. A simple linear regression can not be used for classification as the output of a linear regression can have a range that goes from $-\infty$ to $\infty$ (we need to find the values in the range [0, 1]). Instead, we can transform the output of linear regression such that the output is confined in the range [0, 1].</description>
    </item>
    
    <item>
      <title>ISLR Chapter 4: Classification (Part 1: Logistic Regression)</title>
      <link>https://amitrajan012.github.io/post/classification_part1/</link>
      <pubDate>Sun, 13 May 2018 02:17:58 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/classification_part1/</guid>
      <description>Classification A process for predicting qualitative or categorical variables is called as Classification.
4.1 An Overview of Classification The dataset used in this chapter will be Default dataset. We will predict that whether an individual will default on his/her credit card payment on the basis of annual income and monthly credit card balance. The data is displayed below:
import pandas as pd import seaborn as sns import matplotlib.pyplot as plt default = pd.</description>
    </item>
    
  </channel>
</rss>
