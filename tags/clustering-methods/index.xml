<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Clustering Methods on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/clustering-methods/</link>
    <description>Recent content in Clustering Methods on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jul 2018 13:01:01 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/clustering-methods/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 4: Clustering Methods, Hierarchical Clustering)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part4/</link>
      <pubDate>Thu, 12 Jul 2018 13:01:01 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part4/</guid>
      <description>10.3.2 Hierarchical Clustering K-means clustering has a disadvantage that there is a need to pre-specify the number of clusters $K$. Hierarchical clutsring is an alternative approach which is free from this problem which results in an altarnative tree-based representation of the observations, called as dendrogram.
The most common technique used for hierarchical clustering is bottom-up or agglomerative clustering. It is based on the fact that the dendrogram (generally depicted as an upside-down tree) is built starting from leaves and combining the clusters up to the trunk.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 3: Clustering Methods, K-Means Clustering)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part3/</link>
      <pubDate>Mon, 09 Jul 2018 17:09:41 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part3/</guid>
      <description>10.3 Clustering Methods Clustering is a technique for finding subgroups or clusters in a data set based on similarity between individual observations. For clustering, we need to define the measure of similarity which depends on the knowledge of the data set. Two best known clustering methods are K-means clustering and hierarchical clustering. In K-means clustering, we partition the observations into a pre-defined number of clusters. In hierarchical clustering, the number of clusters is unknown and the results of clustering is represented as a dendrogram, which is a tree-like visualization technique that allows us to view the clustering results for various number of clusters (from 1 to $n$).</description>
    </item>
    
  </channel>
</rss>
