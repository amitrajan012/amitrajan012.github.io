<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tree-Based Methods on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/tree-based-methods/</link>
    <description>Recent content in Tree-Based Methods on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 16 Jun 2018 01:34:53 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/tree-based-methods/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 4: Exercises - Applied)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part4/</link>
      <pubDate>Sat, 16 Jun 2018 01:34:53 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part4/</guid>
      <description>Applied Q 7. In the lab, we applied random forests to the Boston data using mtry=6 and using ntree=25 and ntree=500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained.
import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error from sklearn.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part3/</link>
      <pubDate>Thu, 14 Jun 2018 07:14:13 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part3/</guid>
      <description>8.4 Exercises Conceptual Q2. It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form
$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$
Explain why this is the case. You can begin with (8.12) in Algorithm 8.2.
Sol: As for depth-one trees, value of $d$ is 1. Each tree is generated by splitting the data on only one predictor and the final model is formed by adding the shrunken version of them repeatedly.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 2: Bagging, Random Forests, Boosting)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part2/</link>
      <pubDate>Wed, 13 Jun 2018 13:04:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part2/</guid>
      <description>8.2 Bagging, Random Forests, Boosting 8.2.1 Bagging The decision trees discussed above suffers from a problem of high variance. Bootstrap aggregation or bagging is a procedure that reduces the variance of a statistical learning method.
Give a set of $n$ independent observation sets $Z_1, Z_2, &amp;hellip;, Z_n$, each with variance $\sigma^2$, the variance of the mean $\bar{Z}$ is given by $\sigma^2/n$, i.e. averaging a set of observations reduces variance. Hence, a natural way to reduce the variance of a statistical model is to take many training samples from the population, fit individual models on them, and give the average of them as the final model.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 1: Decision Trees)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part1/</link>
      <pubDate>Mon, 11 Jun 2018 17:07:13 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part1/</guid>
      <description>Tree-Based Methods Tree-based methods stratify or segment the predictor space into a number of simple regions. To make a prediction, we use the mean or the mode of the training observations in the region in which the observation to be predicted belongs. The set of splitting rules can be summarized via a tree, these methods are also known as decision tree methods. Bagging, random forests and boosting produce multiple trees and then combine them in a single model to make the prediction.</description>
    </item>
    
  </channel>
</rss>
