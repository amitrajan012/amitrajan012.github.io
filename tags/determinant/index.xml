<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Determinant on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/determinant/</link>
    <description>Recent content in Determinant on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Apr 2022 14:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/determinant/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Symmetric Matrices and Positive Definiteness</title>
      <link>https://amitrajan012.github.io/post/chapter_22_symmetric_matrices_and_positive_definiteness/</link>
      <pubDate>Tue, 26 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_22_symmetric_matrices_and_positive_definiteness/</guid>
      <description>22.1 Symmetric Matrices For a Symmetric Matrix $A$, $A = A^T$. Eigenvalues of real Symmetric Matrices are real and eigenvectors are perpendicular. Any matrix $A$ can be written as $A = S\Lambda S^{-1}$. For a symmetric matrix $A$, this relationship reduces to $A = Q \Lambda Q^{-1}$ as $S$, which is the eigenvector matrix has orthonormal eigenvectors. For an orthonormal matrix, $Q^{-1} = Q^T$ and hence $A = Q\Lambda Q^T$. This is called as the Spectral Theorem in mathematics.</description>
    </item>
    
    <item>
      <title>Markov Matrices and Fourier Series</title>
      <link>https://amitrajan012.github.io/post/chapter_21_markov_matrices_and_fourier_series/</link>
      <pubDate>Fri, 22 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_21_markov_matrices_and_fourier_series/</guid>
      <description>21.1 Markov Matrices Markov Matrices have the following properties:
 All entries $\geq 0$ Sum of the entries in a column equal $1$  A markov matrix will always have an eigenvalue of $1$. Apart from this, all other eigenvalues will be $\leq 1$.
Let us consider the difference equation $u_k = A^ku_0$ where we represent $u_0$ as the combinations of eigenvectors, i.e. $u_0 = c_1x_1 + c_2x_2 + &amp;hellip; + c_nx_n = Sc$.</description>
    </item>
    
    <item>
      <title>Differential Equations and Matrix Exponentials</title>
      <link>https://amitrajan012.github.io/post/chapter_20_differential_equations_and_expat/</link>
      <pubDate>Wed, 20 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_20_differential_equations_and_expat/</guid>
      <description>20.1 Differential Equations $\frac{du}{dt} = Au$ Example: Let the system of differential equation to be solved is: $\frac{du_1}{dt} = -u_1 + 2u_2; \frac{du_2}{dt} = u_1 - 2u_2$ with initial condition of $u(0) = \begin{bmatrix} 1 \\ 0 \end{bmatrix}$. The matrix $A$ representing the coefficients of the equation is $A = \begin{bmatrix} -1 &amp;amp; 2 \\ 1 &amp;amp; -2 \end{bmatrix}$. The eigenvalues of the matrix $A$ satisfies the equation $\lambda_1 + \lambda_2 = -3; \lambda_1 \times \lambda_2 = 0$, i.</description>
    </item>
    
    <item>
      <title>Diagonalization and Powers of a Matrix</title>
      <link>https://amitrajan012.github.io/post/chapter_19_diagonalization_and_powers_of_a_matrix/</link>
      <pubDate>Sun, 17 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_19_diagonalization_and_powers_of_a_matrix/</guid>
      <description>19.1 Diagonalization of a Matrix Suppose for a given matrix $A$, we have $n$ linearlly independent eigenvectros and we put them in a matrix $S$. Then $AS = A \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix}$, where each $x_i$ is the eigenvector. But for each of the eigenvectors, $Ax_i = \lambda_ix_i$. Hence, $AS = A \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix} = \begin{bmatrix} \lambda_1x_1 &amp;amp; \lambda_2x_2 &amp;amp; &amp;hellip; &amp;amp; \lambda_nx_n \end{bmatrix} = \begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; &amp;hellip; &amp;amp; x_n \end{bmatrix}diag(\lambda_i) = S\Lambda$.</description>
    </item>
    
    <item>
      <title>Eigenvalues and Eigenvectors</title>
      <link>https://amitrajan012.github.io/post/chapter_18_eigenvalues_and_eigenvectors/</link>
      <pubDate>Thu, 14 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_18_eigenvalues_and_eigenvectors/</guid>
      <description>18.1 Eigenvalues and Eigenvectors The usual function of a matrix is to act on a vector like a function, i.e. in goes a vector $x$, out comes a vector $Ax$. For a specific matrix $A$, if $Ax$ is parallel to $x$ i.e. $Ax = \lambda x$, the vectors $x$ are called as eigenvectors. The constant $\lambda$ is called as eigenvalue.
The vector $x$ and constant $\lambda$ satisfying the equation $Ax = \lambda x$ are eigenvectors and eigenvalues.</description>
    </item>
    
    <item>
      <title>Formula for $A^{-1}$ and Cramer&#39;s Rule</title>
      <link>https://amitrajan012.github.io/post/chapter_17_cramers_rule_inverse_matrix_and_volume/</link>
      <pubDate>Sun, 10 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_17_cramers_rule_inverse_matrix_and_volume/</guid>
      <description>17.1 Formula for $A^{-1}$ $A^{-1}$ can be given as $A^{-1} = \frac{1}{|A|}C^T$, where $C^T$ is the matrix of cofactors transposed. To verify this formula, we have to check that $AA^{-1}=I$, or $AC^T=|A|I$. If we expand the left hand side, we get
$$\begin{align} \begin{bmatrix} a_{11} &amp;amp; &amp;hellip; &amp;amp; a_{1n}\\ : &amp;amp; : &amp;amp; :\\ a_{n1} &amp;amp; &amp;hellip; &amp;amp; a_{nn} \end{bmatrix}\begin{bmatrix} C_{11} &amp;amp; &amp;hellip; &amp;amp; C_{n1}\\ : &amp;amp; : &amp;amp; :\\ C_{1n} &amp;amp; &amp;hellip; &amp;amp; C_{nn} \end{bmatrix}=\begin{bmatrix} |A| &amp;amp; 0 &amp;amp; 0\\ 0 &amp;amp; |A| &amp;amp; 0\\ 0 &amp;amp; 0 &amp;amp; |A| \end{bmatrix}=|A|I \end{align}$$</description>
    </item>
    
    <item>
      <title>Determinant and Cofactors</title>
      <link>https://amitrajan012.github.io/post/chapter_16_determinant_formulas_and_cofactors/</link>
      <pubDate>Thu, 07 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_16_determinant_formulas_and_cofactors/</guid>
      <description>16.1 Formula for $|A|$ For a $2 \times 2$ matrix $A$, the formula for $|A|$ can be derived as follows:
$$\begin{align} |A| = \begin{vmatrix} a &amp;amp; b \\ c &amp;amp; d \end{vmatrix}= \begin{vmatrix} a &amp;amp; 0 \\ c &amp;amp; d \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ c &amp;amp; d \end{vmatrix} \end{align}$$
$$\begin{align} =\begin{vmatrix} a &amp;amp; 0 \\ c &amp;amp; 0 \end{vmatrix}+\begin{vmatrix} a &amp;amp; 0 \\ 0 &amp;amp; d \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ c &amp;amp; 0 \end{vmatrix}+\begin{vmatrix} 0 &amp;amp; b \\ 0 &amp;amp; d \end{vmatrix} \end{align}$$</description>
    </item>
    
    <item>
      <title>Determinant</title>
      <link>https://amitrajan012.github.io/post/chapter_15_properties_of_determinants/</link>
      <pubDate>Tue, 05 Apr 2022 14:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/chapter_15_properties_of_determinants/</guid>
      <description>15.1 Determinant Every square matrix has a number assosiated with it can we call this number as determinant, often denoted as $det(A) = |A|$ for matrix $A$.
Prperties of determinant is as follows:
  $|I|=1$
  Row exchange reverses the sign of the determinant:
  Permutation Matrices are derived by row exchange of Identity Matrix. Hence, $|P|= \pm1$.
 (a) For any square matrix $A$, $\begin{vmatrix} ta &amp;amp; tb \\ c &amp;amp; d \end{vmatrix}=t\begin{vmatrix} a &amp;amp; b \\ c &amp;amp; d \end{vmatrix}$</description>
    </item>
    
  </channel>
</rss>
