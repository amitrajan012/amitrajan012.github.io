<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Jacobian Matrix on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/jacobian-matrix/</link>
    <description>Recent content in Jacobian Matrix on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Jul 2022 23:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/jacobian-matrix/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Neural Networks - Error Backpropagation</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-5-neural-networks_3/</link>
      <pubDate>Wed, 13 Jul 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-5-neural-networks_3/</guid>
      <description>5.3 Error Backpropagation Our goal in this section is to find an efficient technique for evaluating the gradient of an error function $E(W)$ for a feed-forward neural network. We shall see that this can be achieved using a local message passing scheme in which information is sent alternately forwards and backwards through the network and is known as error backpropagation, or sometimes simply as backprop.
Most training algorithms involve an iterative procedure for minimization of an error function, with adjustments to the weights being made in a sequence of steps.</description>
    </item>
    
  </channel>
</rss>
