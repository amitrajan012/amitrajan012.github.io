<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Conceptual on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/conceptual/</link>
    <description>Recent content in Conceptual on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Jul 2018 03:51:27 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/conceptual/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ISLR Chapter 10: Unsupervised Learning (Part 5: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/unsupervised-learning_part5/</link>
      <pubDate>Sun, 15 Jul 2018 03:51:27 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/unsupervised-learning_part5/</guid>
      <description>10.7 Exercises Conceptual Q1. This problem involves the K-means clustering algorithm.
(a) Prove (10.12).
Sol: Equation 10.12 is:
$$\frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} (x _{ij} - x _{i^{&amp;rsquo;}j})^2 = 2\sum _{i \in C_k} \sum _{j=1}^{p} (x _{ij} - \bar{x} _{kj})^2$$
where $\bar{x} _{kj} = \frac{1}{|C_k|} \sum _{i \in C_k} x _{ij}$, is the mean of feature $j$ in cluster $C_k$. Expanding LHS, we get
$$\frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} (x _{ij} - x _{i^{&amp;rsquo;}j})^2 = \frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{ij}^2 + \frac{1}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{i^{&amp;rsquo;}j}^2 - \frac{2}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{ij}x _{i^{&amp;rsquo;}j} \ = 2 \sum _{i \in C_k} \sum _{j=1}^{p} x _{ij}^2 - \frac{2}{|C_k|} \sum _{i, i^{&amp;rsquo;} \in C_k} \sum _{j=1}^{p} x _{ij}x _{i^{&amp;rsquo;}j} $$</description>
    </item>
    
    <item>
      <title>ISLR Chapter 9: Support Vector Machines (Part 4: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/support-vector-machines_part4/</link>
      <pubDate>Tue, 26 Jun 2018 03:29:17 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/support-vector-machines_part4/</guid>
      <description>9.7 Exercises Conceptual Q1. This problem involves hyperplanes in two dimensions.
(a) Sketch the hyperplane 1 + 3X1 − X2 = 0. Indicate the set of points for which 1 + 3X1 − X2 &amp;gt; 0, as well as the set of points for which 1 + 3X1 − X2 &amp;lt; 0.
(b) On the same plot, sketch the hyperplane −2 + X1 + 2X2 = 0. Indicate the set of points for which −2+ X1 +2X2 &amp;gt; 0, as well as the set of points for which −2+ X1 + 2X2 &amp;lt; 0.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 8: Tree-Based Methods (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/tree-based-methods_part3/</link>
      <pubDate>Thu, 14 Jun 2018 07:14:13 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/tree-based-methods_part3/</guid>
      <description>8.4 Exercises Conceptual Q2. It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form
$$f(X) = \sum_{j=1}^{p} f_j(X_j)$$
Explain why this is the case. You can begin with (8.12) in Algorithm 8.2.
Sol: As for depth-one trees, value of $d$ is 1. Each tree is generated by splitting the data on only one predictor and the final model is formed by adding the shrunken version of them repeatedly.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 7: Moving Beyond Linearity (Part 5: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/moving-beyond-linearity_part5/</link>
      <pubDate>Wed, 06 Jun 2018 06:27:22 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/moving-beyond-linearity_part5/</guid>
      <description>7.9 Exercises Conceptual Q1. It was mentioned in the chapter that a cubic regression spline with one knot at ξ can be obtained using a basis of the form $x, x^2, x^3, (x − ξ)^3 _+$, where $(x − ξ)^3 _+ = (x − ξ)^3$ if x &amp;gt; ξ and equals 0 otherwise. We will now show that a function of the form $f(x) = β_0 + β_1x + β_2x^2 + β_3x^3 + β_4(x − ξ)^3 _+$ is indeed a cubic regression spline, regardless of the values of $β_0, β_1, β_2, β_3, β_4$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 6: Linear Model Selection and Regularization (Part 4: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part4/</link>
      <pubDate>Fri, 25 May 2018 06:18:38 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-model-selection-and-regularization_part4/</guid>
      <description>6.8 Exercises Conceptual Q1. We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain p + 1 models, containing 0, 1, 2, . . . , p predictors. Explain your answers:
(a) Which of the three models with k predictors has the smallest training RSS?
Sol: Training RSS is minimum for best subset selection.
(b) Which of the three models with k predictors has the smallest test RSS?</description>
    </item>
    
    <item>
      <title>ISLR Chapter 5: Resampling Methods (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/resampling-methods_part3/</link>
      <pubDate>Fri, 18 May 2018 07:18:30 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/resampling-methods_part3/</guid>
      <description>5.4 Exercises Conceptual Q1. Using basic statistical properties of the variance, as well as singlevariable calculus, derive that the value of $\alpha$ which minimizes $Var(\alpha X + (1 - \alpha) Y)$ is:
$$\alpha = \frac{\sigma_Y^2 - \sigma _{XY}}{\sigma_X^2 + \sigma_Y^2 - 2\sigma _{XY}}$$
Sol: As we know that $Var(aX + bY) = a^2 Var(X) + b^2 Var(Y) + 2abCov(X, Y)$, the above quantity (that needs to be minimized) can be transformed as:</description>
    </item>
    
    <item>
      <title>ISLR Chapter 4: Classification (Part 3: Exercises- Conceptual)</title>
      <link>https://amitrajan012.github.io/post/classification_part3/</link>
      <pubDate>Tue, 15 May 2018 09:02:08 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/classification_part3/</guid>
      <description>4.7 Exercises Conceptual Q1. Using a little bit of algebra, prove that the logistic function representation and logit representation for the logistic regression model are equivalent.
Sol: Logistic function representation is given as:
$$p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}$$
then $1-p(X) = \frac{1}{1 + e^{\beta_0 + \beta_1X}}$, Taking the ratio of these two and then taking the log, we get
$$log\bigg( \frac{p(X)}{1-p(X)} \bigg) = \beta_0 + \beta_1X$$</description>
    </item>
    
    <item>
      <title>ISLR Chapter 3: Linear Regression (Part 4: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/linear-regression_part4/</link>
      <pubDate>Thu, 10 May 2018 10:28:18 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/linear-regression_part4/</guid>
      <description>3.7 Exercises Conceptual Solution: The linear fit can be given as:
$$50 + (20 \times GPA) + (0.07 \times IQ) + (35 \times GENDER) + (0.01 \times GPA \times IQ) - (10 \times GPA \times GENDER)$$
(a) For a fixed value of IQ and GPA, the average salary for male will be $50 + 20 \times GPA$ and for the female it will be $85 + 20 \times GPA - 10 \times GPA$.</description>
    </item>
    
    <item>
      <title>ISLR Chapter 2: Statistical Learning (Part 3: Exercises - Conceptual)</title>
      <link>https://amitrajan012.github.io/post/statistical-learning_part3/</link>
      <pubDate>Sun, 06 May 2018 01:38:54 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/statistical-learning_part3/</guid>
      <description>2.4 Exercises Conceptual Q1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. (a) The sample size n is extremely large, and the number of predic- tors p is small.
Sol: Better
(b) The number of predictors p is extremely large, and the number of observations n is small.</description>
    </item>
    
  </channel>
</rss>
