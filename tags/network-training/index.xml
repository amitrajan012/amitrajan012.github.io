<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Network Training on Amit Rajan</title>
    <link>https://amitrajan012.github.io/tags/network-training/</link>
    <description>Recent content in Network Training on Amit Rajan</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Jul 2022 23:07:28 +0100</lastBuildDate><atom:link href="https://amitrajan012.github.io/tags/network-training/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Neural Networks - Error Backpropagation</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-5-neural-networks_3/</link>
      <pubDate>Wed, 13 Jul 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-5-neural-networks_3/</guid>
      <description>5.3 Error Backpropagation Our goal in this section is to find an efficient technique for evaluating the gradient of an error function $E(W)$ for a feed-forward neural network. We shall see that this can be achieved using a local message passing scheme in which information is sent alternately forwards and backwards through the network and is known as error backpropagation, or sometimes simply as backprop.
Most training algorithms involve an iterative procedure for minimization of an error function, with adjustments to the weights being made in a sequence of steps.</description>
    </item>
    
    <item>
      <title>Neural Networks - Network Training</title>
      <link>https://amitrajan012.github.io/post/pattern-recognition-chapter-5-neural-networks_2/</link>
      <pubDate>Mon, 11 Jul 2022 23:07:28 +0100</pubDate>
      
      <guid>https://amitrajan012.github.io/post/pattern-recognition-chapter-5-neural-networks_2/</guid>
      <description>5.2 Network Training A simple approach to the problem of determining the network parameters is to make an analogy with the discussion of polynomial curve fitting, and therefore to minimize a sum-of-squares error function. Given a training set comprising a set of input vectors ${X_n}$, where $n=1,2,&amp;hellip;,N$ with the corresponding output set as ${t_n}$, we minimize the error function
$$\begin{align} E(W) = \frac{1}{2}\sum_{n=1}^{N}||y_n(X_n,W) - t_n||^2 \end{align}$$
Let us consider a regression case with single target variable $t$ that can take any real value.</description>
    </item>
    
  </channel>
</rss>
